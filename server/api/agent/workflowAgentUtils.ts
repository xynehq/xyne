import { z } from "zod"
import { db } from "@/db/client"
import { getAgentByExternalId, type SelectAgent } from "@/db/agent"
import { generateTitleUsingQuery, getProviderByModel } from "@/ai/provider"
import { insertChat } from "@/db/chat"
import { insertMessage } from "@/db/message"
import { Models, type ConverseResponse } from "@/ai/types"
import { getLogger } from "@/logger"
import { getErrorMessage } from "@/utils"
import type { Message, ConversationRole } from "@aws-sdk/client-bedrock-runtime"
import { Subsystem, MessageRole, type UserMetadataType } from "@/types"
import { getUserAndWorkspaceByEmail } from "@/db/user"
import { GetDocumentsByDocIds } from "@/search/vespa" // Retrieve non-image attachments from Vespa
import { answerContextMap, cleanContext } from "@/ai/context" // Transform Vespa results to text
import { VespaSearchResultsSchema } from "@xyne/vespa-ts/types" // Type for Vespa results
import { getTracer, type Span } from "@/tracer"
import { createAgentSchema } from "@/api/agent"
import type { CreateAgentPayload } from "@/api/agent"
import { insertAgent } from "@/db/agent"
import { getDateForAI } from "@/utils/index"
import { AgentCreationSource } from "@/db/schema"
import { UnderstandMessageAndAnswer } from "@/api/chat/chat"
import { generateSearchQueryOrAnswerFromConversation, jsonParseLLMOutput } from "@/ai/provider"
import type { Citation, ImageCitation } from "@/shared/types"
import { getAgentByExternalIdWithPermissionCheck } from "@/db/agent"
import type { QueryRouterLLMResponse } from "@/ai/types"
import { agentWithNoIntegrationsQuestion } from "@/ai/provider"
import config from "@/config"
import { exec } from "child_process"
import { de } from "zod/v4/locales"
const {
  defaultBestModel,
} = config

const Logger = getLogger(Subsystem.Server)

export const executeAgentSchema = z.object({
  agentId: z.string().min(1, "Agent ID is required"),
  userQuery: z.string().min(1, "User query is required"),
  workspaceId: z.string().min(1, "Workspace ID is required"),
  userEmail: z.string().email("Valid email is required"),
  isStreamable: z.boolean().optional().default(false),
  temperature: z.number().min(0).max(2).optional(),
  max_new_tokens: z.number().positive().optional(),
  attachmentFileIds: z.array(z.string()).optional().default([]),        // For images: ["att_123", "att_456"]
  nonImageAttachmentFileIds: z.array(z.string()).optional().default([]), // For PDFs: ["att_789"]
})


export type ExecuteAgentParams = z.infer<typeof executeAgentSchema>

type ExecuteAgentSuccess = {
  success: true
  chatId: string
  title: string
  agentName: string
  modelId: string
}

type StreamingExecuteAgentResponse = ExecuteAgentSuccess & {
  type: 'streaming'
  iterator: AsyncIterableIterator<ConverseResponse>
}

type NonStreamingExecuteAgentResponse = ExecuteAgentSuccess & {
  type: 'non-streaming'
  response: ConverseResponse
}

type ExecuteAgentErrorResponse = {
  success: false
  error: string
  details?: any
}

export type ExecuteAgentResponse =
  | StreamingExecuteAgentResponse
  | NonStreamingExecuteAgentResponse
  | ExecuteAgentErrorResponse


/**
 * Response from agent execution with full features
 */
export interface ExecuteAgentWithRagResponse {
  success: boolean
  response?: string                    // AI's answer
  citations?: any[]                    // Document citations
  imageCitations?: any[]               // Image citations
  thinking?: string                    // AI's reasoning (if enabled)
  cost?: string                        // API cost
  tokensUsed?: number                  // Token count
  error?: string                       // Error message (if failed)
  chatId?: string | null               // No chat for workflows
  messageId?: string | null            // No message for workflows
}

/**
 * ExecuteAgentForWorkflow - Simplified agent execution function with attachment support
 * 
 * This function provides a simplified subset of AgentMessageApi functionality:
 * 1. Generate chat title and insert in DB
 * 2. Fetch agent details from agent table (includes model)
 * 3. Process attachments (images and documents)
 * 4. Call LLM directly with agent prompt + user query + attachments
 * 5. Return response (no reasoning loop, no RAG, no tools)
 */
export const ExecuteAgentForWorkflow = async (params: ExecuteAgentParams): Promise<ExecuteAgentResponse> => {
  try {
    // Validate parameters
    const validatedParams = executeAgentSchema.parse(params)
    const tracer = getTracer("executeAgent")
    const executeAgentSpan = tracer.startSpan('executeAgent')
    const {
      agentId,
      userQuery,
      isStreamable = true,
      temperature,
      max_new_tokens,
      workspaceId,
      userEmail,
      attachmentFileIds = [],
      nonImageAttachmentFileIds = [],
    } = validatedParams

    Logger.info(`üöÄ executeAgent called with parameters:`)
    Logger.info(`   - agentId: ${agentId}`)
    Logger.info(`   - userEmail: ${userEmail}`)
    Logger.info(`   - workspaceId: ${workspaceId}`)
    Logger.info(`   - userQuery length: ${userQuery.length}`)
    Logger.info(`   - isStreamable: ${isStreamable}`)
    Logger.info(`   - attachmentFileIds: ${JSON.stringify(attachmentFileIds)}`)
    Logger.info(`   - nonImageAttachmentFileIds: ${JSON.stringify(nonImageAttachmentFileIds)}`)

    const userAndWorkspace = await getUserAndWorkspaceByEmail(
      db,
      workspaceId,
      userEmail,
    )


    const { user, workspace } = userAndWorkspace
    const userTimezone = user?.timeZone || "Asia/Kolkata"
    const dateForAI = getDateForAI({ userTimeZone: userTimezone })
    const userMetadata: UserMetadataType = { userTimezone, dateForAI }
    Logger.info(`Fetched user: ${user.id} and workspace: ${workspace.id}`)

    Logger.info(`Fetching agent details for ${agentId}...`)
    const agent = await getAgentByExternalId(db, agentId, Number(workspace.id))

    if (!agent) {
      return {
        success: false,
        error: `Agent with ID ${agentId} not found`
      }
    }

    if (!agent.model) {
      return {
        success: false,
        error: `Agent ${agentId} has no model configured`
      }
    }

    Logger.info(`Found agent: ${agent.name} with model: ${agent.model}`)

    // ========================================
    // ATTACHMENT PROCESSING SECTION
    // ========================================

    let contextualContent = ""
    let finalImageFileNames: string[] = []

    Logger.info("üîç Starting attachment processing...")
    Logger.info(`üìé Non-image attachments: ${JSON.stringify(nonImageAttachmentFileIds)}`)
    Logger.info(`üñºÔ∏è Image attachments: ${JSON.stringify(attachmentFileIds)}`)

    // Step 1: Handle Non-Image Attachments (PDFs, DOCX, etc.)
    if (nonImageAttachmentFileIds.length > 0) {
      Logger.info(`üìÑ Processing ${nonImageAttachmentFileIds.length} non-image attachments`)

      try {
        // Retrieve document content from Vespa (same as chat.ts:1974-1979)
        Logger.info(`üîç Calling GetDocumentsByDocIds with IDs: ${JSON.stringify(nonImageAttachmentFileIds)}`)

        //fetching document from VESPA
        const results = await GetDocumentsByDocIds(nonImageAttachmentFileIds, executeAgentSpan!)

        Logger.info(`üìä GetDocumentsByDocIds returned: {
          hasRoot: ${!!results.root},
          hasChildren: ${!!(results.root?.children)},
          childrenCount: ${results.root?.children?.length || 0},
        }`)

        if (results.root.children && results.root.children.length > 0) {
          Logger.info(`üìö Found ${results.root.children.length} documents, transforming to readable context...`)

          // Transform Vespa results to readable context (same as chat.ts:2054-2120)
          const contextPromises = results.root.children.map(async (v, i) => {
            Logger.info(`üìñ Processing document ${i} with ID: ${v.id}`)

            const content = await answerContextMap(
              v as z.infer<typeof VespaSearchResultsSchema>,
              userMetadata,
              0,    // maxSummaryChunks (0 = include all chunks)
              true, // isSelectedFiles
            )

            Logger.info(`üìù Document ${i} processed, content length: ${content.length} characters`)
            return `Index ${i} \n ${content}`
          })

          const resolvedContexts = await Promise.all(contextPromises)
          contextualContent = cleanContext(resolvedContexts.join("\n"))

          Logger.info(`‚úÖ Context building completed!`)
          Logger.info(`üìè Total context length: ${contextualContent.length} characters`)
          Logger.info(`üìÑ Context preview (first 200 chars): ${contextualContent.substring(0, 200)}...`)

        } else {
          Logger.warn("‚ö†Ô∏è No documents found in Vespa results")
        }

      } catch (error) {
        Logger.error(error, "‚ùå Error processing non-image attachments")
        // Continue execution even if attachment processing fails
      }
    } else {
      Logger.info("üìÑ No non-image attachments to process")
    }

    // Step 2: Handle Image Attachments 
    if (attachmentFileIds.length > 0) {
      Logger.info(`üñºÔ∏è Processing ${attachmentFileIds.length} image attachments`)

      // Transform attachment IDs to image file names (same as chat.ts:2127-2131)
      finalImageFileNames = attachmentFileIds.map((fileid, index) => {
        const imageName = `${index}_${fileid}_${0}`  // Format: "0_att_123_0"
        Logger.info(`üè∑Ô∏è Transformed attachment ID "${fileid}" ‚Üí image name "${imageName}"`)
        return imageName
      })

      Logger.info(`üñºÔ∏è Final image file names: ${JSON.stringify(finalImageFileNames)}`)
    } else {
      Logger.info("üñºÔ∏è No image attachments to process")
    }

    Logger.info("‚úÖ Attachment processing completed!")
    Logger.info(`üìä Summary:`)
    Logger.info(`   - Context length: ${contextualContent.length} characters`)
    Logger.info(`   - Image files: ${finalImageFileNames.length}`)

    // ========================================
    // END ATTACHMENT PROCESSING SECTION
    // ========================================

    Logger.info("Generating chat title...")

    const titleResp = await generateTitleUsingQuery(userQuery, {
      modelId: agent.model as Models|| defaultBestModel,
      stream: false,
    })
    const title = titleResp.title
    Logger.info(`Generated title: ${title}`)

    Logger.info("üîß Building model parameters...")

    const modelParams = {
      modelId: agent.model as Models || defaultBestModel,
      stream: isStreamable,
      json: false,
      reasoning: false,
      systemPrompt: (agent.prompt || "You are a helpful assistant.") + "\n\nIMPORTANT: Please provide responses in plain text format only. Do not use markdown.",

      // ADD IMAGE SUPPORT:
      ...(finalImageFileNames.length > 0 ? { imageFileNames: finalImageFileNames } : {}),

      ...(temperature !== undefined ? { temperature } : {}),
      ...(max_new_tokens !== undefined ? { max_new_tokens } : {}),
    }

    Logger.info(`üîß Model parameters built: {
      modelId: ${modelParams.modelId || defaultBestModel},
      hasImages: ${!!(modelParams as any).imageFileNames},
      imageCount: ${((modelParams as any).imageFileNames || []).length},
      systemPromptLength: ${modelParams.systemPrompt.length},
      hasTemperature: ${temperature !== undefined},
      hasMaxTokens: ${max_new_tokens !== undefined},
    }`)

    Logger.info("üí¨ Constructing LLM messages...")  

    // UPDATE MESSAGE CONSTRUCTION TO INCLUDE CONTEXT:
    const userContent = contextualContent
      ? `Context from attached documents:\n${contextualContent}\n\nUser Query: ${userQuery}`  // Include document context
      : userQuery  // No context, just user query

    Logger.info(`üí¨ Message construction details: {
      hasContext: ${!!contextualContent},
      contextLength: ${contextualContent.length},
      userQueryLength: ${userQuery.length},
      finalContentLength: ${userContent.length},
    }`)

    Logger.info("üí¨ Final user content preview (first 300 chars):")
    Logger.info(userContent.substring(0, 300) + (userContent.length > 300 ? "..." : ""))

    const messages: Message[] = [
      {
        role: "user" as ConversationRole,
        content: [{ text: userContent }],  // User query + document context
      }
    ]

    Logger.info("üí¨ Messages array constructed with 1 user message")

    Logger.info(`Calling LLM with model ${agent.model}...`)

    const insertedChat = await insertChat(db, {
      workspaceId: workspace.id,
      workspaceExternalId: workspace.externalId,
      userId: user.id,
      email: userEmail,
      title,
      attachments: [],
      agentId: agent.externalId,
    })

    await insertMessage(db, {
      chatId: insertedChat.id,
      userId: user.id,
      chatExternalId: insertedChat.externalId,
      workspaceExternalId: workspace.externalId,
      messageRole: MessageRole.User,
      email: userEmail,
      sources: [],
      message: userQuery,
      modelId: agent.model,
    })

    // Step 6: Call LLM and return based on streaming preference
    if (isStreamable) {
      Logger.info("üåä Agent execution started (streaming mode)")
      Logger.info(`üåä About to call LLM with attachments: {
        hasImages: ${finalImageFileNames.length > 0},
        imageFiles: ${finalImageFileNames},
        hasContext: ${contextualContent.length > 0},
        contextLength: ${contextualContent.length},
      }`)

      // Add provider debugging
      const provider = getProviderByModel(agent.model as Models || defaultBestModel)
      Logger.info(`ü§ñ Provider for model ${agent.model}: ${typeof provider}`)

      try {
        Logger.info("üîÑ Creating original iterator...")
        const originalIterator = provider.converseStream(messages, modelParams)
        Logger.info("üîÑ Original iterator created, starting wrapper...")

        const wrappedIterator = createStreamingWithDBSave(originalIterator, {
          chatId: insertedChat.id,
          userId: user.id,
          chatExternalId: insertedChat.externalId,
          workspaceExternalId: workspaceId,
          email: userEmail,
          modelId: agent.model,
        })

        Logger.info("‚úÖ Wrapper created successfully")

        return {
          success: true,
          type: 'streaming',
          iterator: wrappedIterator,
          chatId: insertedChat.externalId,
          title,
          agentName: agent.name,
          modelId: agent.model,
        }
      } catch (providerError) {
        Logger.error(providerError, "‚ùå Error creating streaming iterator")
        throw providerError
      }
    } else {
      Logger.info("üí´ Agent execution started (non-streaming mode)")
      Logger.info(`üí´ About to call LLM with attachments: {
        hasImages: ${finalImageFileNames.length > 0},
        imageFiles: ${finalImageFileNames},
        hasContext: ${contextualContent.length > 0},
        contextLength: ${contextualContent.length},
      }`)

      // Get non-streaming response
      const response = await getProviderByModel(agent.model as Models || defaultBestModel).converse(
        messages,
        modelParams
      )

      Logger.info(`üí´ LLM response received: {
        hasText: ${!!response.text},
        textLength: ${response.text?.length || 0},
        hasCost: ${!!response.cost},
        cost: ${response.cost},
      }`)

      if (!response.text) {
        return {
          success: false,
          error: "No response received from LLM"
        }
      }

      await insertMessage(db, {
        chatId: insertedChat.id,
        userId: user.id,
        chatExternalId: insertedChat.externalId,
        workspaceExternalId: workspace.externalId,
        messageRole: MessageRole.Assistant,
        email: userEmail,
        sources: [],
        message: response.text,
        modelId: agent.model,
        cost: response.cost?.toString(),
      })

      Logger.info("‚úÖ Agent execution completed successfully (non-streaming)")

      return {
        success: true,
        type: 'non-streaming',
        chatId: insertedChat.externalId,
        title,
        response: response,
        agentName: agent.name,
        modelId: agent.model,
      }
    }

  } catch (error) {

    Logger.error(error, "Error in executeAgent")

    if (error instanceof z.ZodError) {
      return {
        success: false,
        error: "Invalid request parameters",
        details: error.flatten().fieldErrors,
      }
    }

    return {
      success: false,
      error: getErrorMessage(error),
    }
  }
}

async function* createStreamingWithDBSave(
  originalIterator: AsyncIterableIterator<ConverseResponse>,
  dbSaveParams: {
    chatId: number
    userId: number
    chatExternalId: string
    workspaceExternalId: string
    email: string
    modelId: string
  }
): AsyncIterableIterator<ConverseResponse> {


  Logger.info("üåä createStreamingWithDBSave: Starting...")
  let answer = ""
  let costArr: number[] = []
  let tokenArr: { inputTokens: number; outputTokens: number }[] = []
  let wasStreamClosedPrematurely = false

  try {
    Logger.info("üåä createStreamingWithDBSave: About to start for-await loop...")

    for await (const chunk of originalIterator) {
      if (chunk.text) {
        answer += chunk.text  // Accumulate full response
        yield { text: chunk.text }  // Forward to client
      }
      Logger.info(`üåä createStreamingWithDBSave: Forwarded chunk to client: ${chunk.text}`)

      if (chunk.cost) {
        costArr.push(chunk.cost)  // Accumulate costs
        yield { cost: chunk.cost }
      }

      if (chunk.metadata?.usage) {
        tokenArr.push({  // Accumulate token usage
          inputTokens: chunk.metadata.usage.inputTokens,
          outputTokens: chunk.metadata.usage.outputTokens,
        })
        yield { metadata: chunk.metadata }
      }
    }

    Logger.info("üåä createStreamingWithDBSave: Iterator completed, saving to DB...")

    // Save to DB after stream completes (same pattern as AgentMessageApi)
    if (answer || wasStreamClosedPrematurely) {
      const totalCost = costArr.reduce((sum, cost) => sum + cost, 0)
      const totalTokens = tokenArr.reduce(
        (sum, tokens) => sum + tokens.inputTokens + tokens.outputTokens,
        0,
      )


      await insertMessage(db, {
        chatId: dbSaveParams.chatId,
        userId: dbSaveParams.userId,
        chatExternalId: dbSaveParams.chatExternalId,
        workspaceExternalId: dbSaveParams.workspaceExternalId,
        messageRole: MessageRole.Assistant,
        email: dbSaveParams.email,
        sources: [],
        message: answer,  // Full accumulated text
        modelId: dbSaveParams.modelId,
        cost: totalCost.toString(),
        tokensUsed: totalTokens,
      })

      Logger.info("Assistant message saved to database after streaming")
    }

  } catch (error) {
    Logger.error(error, "Error during streaming or DB save")
    throw error
  }
}


export const executeAgentForWorkflowWithRag = async (
    params: ExecuteAgentParams
): Promise<ExecuteAgentWithRagResponse> => {
    try {
        Logger.info(`[agentCore] Starting execution for agent ${params.agentId}`)

         // Validate with Zod schema
          const validatedParams = executeAgentSchema.parse(params)

        // Initialize tracer for performance monitoring
        const tracer = getTracer("agentCore")
        const executeAgentSpan = tracer.startSpan('executeAgentCore')

        // ============================================
        // STEP 1: Load User, Workspace, and Agent
        // ============================================

        const userAndWorkspace = await getUserAndWorkspaceByEmail(
            db,
            params.workspaceId,
            params.userEmail,
        )
        const { user, workspace } = userAndWorkspace

        Logger.info(`[agentCore] Resolved user ID: ${user.id}, workspace ID: ${workspace.id}`)

        const agent = await getAgentByExternalIdWithPermissionCheck(
            db,
            params.agentId,
            workspace.id,
            user.id,
        )
        //here add attachements id's in app_integrations : google_drive : [...attachmentFileIds]

        if (!agent) {
            Logger.error(`[agentCore] Agent ${params.agentId} not found or permission denied`)
            return {
                success: false,
                error: `Access denied: You don't have permission to use agent ${params.agentId}`,
            }
        }

        Logger.info(`[agentCore] Loaded agent: ${agent.name}`)
        Logger.info(`[agentCore] Agent features: RAG=${agent.isRagOn}, Integrations=${agent.appIntegrations?.length || 0}`)

        // ============================================
        const userTimezone = user?.timeZone || "Asia/Kolkata"
        const dateForAI = getDateForAI({ userTimeZone: userTimezone })
        const userMetadata: UserMetadataType = { userTimezone, dateForAI }

        Logger.info(`[agentCore] User timezone: ${userTimezone}, Date for AI: ${dateForAI}`)

        let contextualContent = ""
      

        // Extract KB from BOTH sources:
        const agentKBDocs = agent.docIds || []
        const directDocIds = agentKBDocs.map((doc) =>
            typeof doc === 'string' ? doc : doc.docId
        )

        // Extract KB from app_integrations
        let kbIntegrationDocIds: string[] = []
        if (agent.appIntegrations && typeof agent.appIntegrations === 'object') {
            const integrations = agent.appIntegrations as Record<string, any>
            if (integrations.knowledge_base && integrations.knowledge_base.itemIds) {
                kbIntegrationDocIds = integrations.knowledge_base.itemIds
            }
        }

        // Combine ALL sources
        const agentKBDocIds = [...directDocIds, ...kbIntegrationDocIds]

        // Merge with user attachments
        const uniqueFileIds = Array.from(new Set([
            ...agentKBDocIds,
        ]))



        Logger.info(`[agentCore] üìä Combined file sources:`)

        Logger.info(`[agentCore]    - Agent KB docs: ${agentKBDocs.length}`)
        Logger.info(`[agentCore]    - Total unique files: ${uniqueFileIds.length}`)

        // Determine if we should use RAG
        const shouldUseRAG = agent.isRagOn && uniqueFileIds.length > 0


        // ============================================
        // TODO: Step 4 - Perform RAG search or direct LLM call
        // ===========================================



        let finalAnswer = ""
        let citations: Citation[] = []
        let imageCitations: ImageCitation[] = []
        let thinking = ""
        let totalCost = 0
        let totalTokens = 0



        if (shouldUseRAG) {
            // ===== Path 1: RAG Pipeline =====
            // Why? Agent has RAG enabled and files available
            // We search the documents and use context to answer

            Logger.info(`[agentCore] üîç Starting RAG pipeline...`)

            const ragSpan = executeAgentSpan?.startSpan("rag_processing")
            const understandSpan = ragSpan?.startSpan("understand_message")

            // Call RAG function (async generator that yields chunks)

            const classification = await classifyUserQuery(
                params.userQuery,
                params.userEmail,
                userMetadata,
                agent.model === "Auto" ? undefined : agent.model,
            )
                  Logger.info(`[agentCore] üìä Classification result: ${JSON.stringify(classification)}`)


                  // this UnderstandMessageAndAnswerForGivenContext is only for the attachment files and do Rag on them not consider the app_integrations

            // const iterator = UnderstandMessageAndAnswerForGivenContext(
            //     params.userEmail,                     // email
            //     params.userQuery,                     // userCtx (context from user)
            //     userMetadata,                         // timezone, date
            //     params.userQuery,                     // message (the question)
            //     0.5,                                  // alpha (search confidence threshold)
            //     uniqueFileIds,                        // fileIds (documents to search)
            //     false,                                // userRequestsReasoning
            //     understandSpan,                       // tracing span
            //     [],                                   // threadIds (empty for workflows)
            //     imageAttachmentFileIds,               // image attachments
            //     agent.prompt || undefined,   // agent's system prompt
            //     true,                                 // isMsgWithSources
            //     agent.model === "Auto" ? undefined : agent.model,             // model ID
            //     undefined,                            // isValidPath
            //     [],                                   // folderIds
            // )

            // STEP 2: Call UnderstandMessageAndAnswer
            // Why? This function properly extracts KB from app_integrations
            // It internally calls generateIterativeTimeFilterAndQueryRewrite which parses the agent JSON
            const iterator = UnderstandMessageAndAnswer(
                params.userEmail,           // email
                params.userEmail,           // userCtx (user context - can be same as email for workflows)
                userMetadata,               // timezone, date
                params.userQuery,           // message (the question)
                classification,             // QueryRouterLLMResponse from classifyUserQuery
                [],                         // messages (empty array for workflows - no conversation history)
                0.5,                        // alpha (search confidence threshold)
                false,                      // userRequestsReasoning
                understandSpan,             // tracing span
                JSON.stringify(agent),      // agentPrompt - CRITICAL: Must be stringified full agent with app_integrations!
                agent.model === "Auto" ? undefined : agent.model,  // modelId
                undefined,                  // pathExtractedInfo (not needed for workflows)
            )


            // Iterate through response chunks
            // Why? The function streams chunks (text, citations, costs)
            for await (const chunk of iterator) {
                if (chunk.text) {
                    // Regular answer text
                    if (!chunk.reasoning) {
                        finalAnswer += chunk.text
                    } else {
                        // Thinking/reasoning text (if enabled)
                        thinking += chunk.text
                    }
                }

                if (chunk.citation) {
                    // Document citation
                    citations.push(chunk.citation.item)
                }

                if (chunk.imageCitation) {
                    // Image citation
                    imageCitations.push(chunk.imageCitation)
                }

                if (chunk.cost) {
                    // API cost
                    totalCost += chunk.cost
                }

                if (chunk.metadata?.usage) {
                    // Token usage
                    totalTokens += chunk.metadata.usage.inputTokens + chunk.metadata.usage.outputTokens
                }
            }

            understandSpan?.end()
            ragSpan?.end()

            Logger.info(`[agentCore] ‚úÖ RAG pipeline completed`)
            Logger.info(`[agentCore] üìä Answer length: ${finalAnswer.length} characters`)
            Logger.info(`[agentCore] üìö Citations: ${citations.length}`)
            Logger.info(`[agentCore] üí∞ Cost: $${totalCost.toFixed(6)}`)

        } else {
            // ===== Path 2: Direct LLM Call =====
            // Why? RAG is disabled or no files available
            // We just call the LLM with the agent's prompt

            Logger.info(`[agentCore] ü§ñ Starting direct LLM call (no RAG)...`)

            const llmSpan = executeAgentSpan?.startSpan("direct_llm_call")

       

            const result = await ExecuteAgentForWorkflow({
                agentId: params.agentId,
                userQuery: params.userQuery,
                userEmail: params.userEmail,
                workspaceId: params.workspaceId,
                isStreamable: false,
                attachmentFileIds: params.attachmentFileIds || [],
                nonImageAttachmentFileIds: params.nonImageAttachmentFileIds || [],
                temperature: params.temperature,

            })

            Logger.info(`[agentCore] Direct LLM call result: ${JSON.stringify(result).substring(0, 700)}...`)
            Logger.info(`[agentCore] ‚úÖ Direct LLM call completed`)
            Logger.info(`[agentCore] üìä Answer length: ${finalAnswer.length} characters`)
            Logger.info(`[agentCore] üí∞ Cost: $${totalCost.toFixed(6)}`)
        }

        executeAgentSpan?.end()

        // Return final response
        return {
            success: true,
            response: finalAnswer,
            citations: citations.length > 0 ? citations : undefined,
            imageCitations: imageCitations.length > 0 ? imageCitations : undefined,
            thinking: thinking || undefined,
            cost: totalCost > 0 ? totalCost.toFixed(6) : undefined,
            tokensUsed: totalTokens,
            chatId: null,      // No chat for workflows
            messageId: null,   // No message for workflows
        }




    } catch (error) {
        Logger.error(error, "[agentCore] Agent execution failed")
        return {
            success: false,
            error: getErrorMessage(error)
        }
    }
}



  /**
   * Classify user query to determine RAG routing strategy
   */
  async function classifyUserQuery(
      userQuery: string,
      userEmail: string,
      userMetadata: UserMetadataType,
      modelId?: string,
  ): Promise<QueryRouterLLMResponse> {
      Logger.info(`[agentCore] üîç Classifying user query...`)

      const classificationIterator = generateSearchQueryOrAnswerFromConversation(
          userQuery,
          userEmail,
          userMetadata,
          {
              modelId: (modelId as Models) || defaultBestModel,
              stream: true,
              json: true,
          },
      )

      let classificationText = ""
      for await (const chunk of classificationIterator) {
          if (chunk.text) {
              classificationText += chunk.text
          }
      }

      const classification = jsonParseLLMOutput(classificationText)

      const result: QueryRouterLLMResponse = {
          type: classification.type || "SearchWithoutFilters",
          direction: classification.direction || null,
          filterQuery: classification.filter_query || null,
          filters: {
              apps: classification.apps || null,
              entities: classification.entities || null,      
              startTime: classification.start_time || null,  
              endTime: classification.end_time || null,      
              sortDirection: classification.sort_direction || null,  
              count: classification.count || 5,              
              offset: classification.offset || 0,             
              intent: classification.intent || undefined,     
          },
      }

      Logger.info(`[agentCore] ‚úÖ Query classified as: ${result.type}`)
      return result
  }


  //this function will be used to be called by workflow feature
export const createAgentForWorkflow = async (
  agentData: CreateAgentPayload,
  userId: number,
  workspaceId: number
): Promise<SelectAgent> => {
  try {
    const validatedBody = createAgentSchema.parse(agentData)

    const agentDataForInsert = {
      name: validatedBody.name,
      description: validatedBody.description,
      prompt: validatedBody.prompt,
      model: validatedBody.model,
      isPublic: validatedBody.isPublic,
      appIntegrations: validatedBody.appIntegrations,
      allowWebSearch: validatedBody.allowWebSearch,
      isRagOn: validatedBody.isRagOn,
      uploadedFileNames: validatedBody.uploadedFileNames,
      docIds: validatedBody.docIds,
      creation_source: AgentCreationSource.WORKFLOW,
    }

    const newAgent = await db.transaction(async (tx) => {
      return await insertAgent(tx, agentDataForInsert, userId, workspaceId)
    })

    return newAgent
  } catch (error) {
    // Re-throw validation errors as-is for the caller to handle
    // Logger.error(error, `Failed to create agent: ${getErrorMessage(error)}`)
    if (error instanceof z.ZodError) {
      throw error
    }

    // Wrap other errors with more context
    const errMsg = getErrorMessage(error)
    throw new Error(`Failed to create agent: ${errMsg}`)
  }
}
