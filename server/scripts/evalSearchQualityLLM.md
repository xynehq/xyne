# LLM-Based Search Quality Evaluation Script (`evaluateSearchQualityLLM.ts`)

This script is designed to evaluate the quality and relevance of search results provided by the Vespa search engine integration, using an LLM to generate realistic search queries based on document content.

## Purpose

The primary goal is to quantitatively measure search effectiveness when queries are generated by an LLM, simulating more complex or conceptual user searches beyond simple keyword matching. This helps answer: "How well does our search system retrieve a specific item when queried with LLM-generated, contextually relevant search terms?"

This is useful for:

*   **Benchmarking:** Establishing a baseline for search performance with LLM-generated queries.
*   **Regression Testing:** Ensuring changes don't negatively impact relevance for more complex query types.
*   **Tuning:** Comparing different search strategies or ranking profiles (especially hybrid search alpha values) against LLM queries.
*   **Failure Analysis:** Optionally using an LLM to analyze *why* certain LLM-generated queries failed to retrieve the target document well.

## Modes of Operation

The script operates in three main modes, controlled by the `EVAL_MODE` environment variable:

1.  **`generate`**: Fetches random documents from Vespa, uses the configured LLM (`OLLAMA_MODEL`) to generate search queries for each, performs verification checks on the queries (using the LLM again), and saves the verified document-query pairs into a JSON dataset file in `server/eval-results/search-quality-llm/`.
2.  **`evaluate`**: Loads a previously generated JSON dataset file, runs the queries from the dataset against Vespa search, calculates retrieval metrics (MRR, NDCG@10, Success@K), and saves a summary report and detailed results.
3.  **`optimize`**: Loads a previously generated JSON dataset file, runs the queries against Vespa search multiple times across a predefined range of hybrid search alpha values (0.0 to 1.0), calculates metrics for each alpha, and reports the best-performing alpha value based on NDCG@10 and MRR.

## Environment Variables

Several environment variables control the script's behavior:

*   `EVAL_MODE`: (Required) Sets the operating mode (`generate`, `evaluate`, `optimize`).
*   `EVALUATION_USER_EMAIL`: (Required) The email address to use for permission checks during Vespa searches.
*   `EVAL_DATASET_PATH`: (Required for `evaluate` and `optimize` modes) Path to the `.json` dataset file generated by a previous `generate` run.
*   `OLLAMA_MODEL`: (Optional, defaults to `config.OllamaModel`) Specifies the Ollama model to use for query generation and analysis (e.g., `qwen3:30b-a3b`, `llama3:latest`).
*   `NUM_SAMPLES`: (Optional, `generate` mode, default: 100) Number of random documents to fetch and generate queries for.
*   `QUERIES_PER_DOC`: (Optional, `generate` mode, default: 1) Number of queries the LLM should attempt to generate for each document.
*   `GENERATE_CONCURRENCY`: (Optional, all modes, default: 5) Number of parallel operations (fetching documents, generating/evaluating queries).
*   `DEFAULT_ALPHA`: (Optional, `evaluate` mode, default: 0.5) The hybrid search alpha value to use when running in `evaluate` mode.
*   `DEBUG_POOR_RANKINGS`: (Optional, `evaluate` mode, default: false) Set to `true` to enable LLM-based analysis of queries that rank the target document poorly (below `POOR_RANK_THRESHOLD`). This will make additional LLM calls.
*   `POOR_RANK_THRESHOLD`: (Optional, `evaluate` mode, default: 10) The rank threshold below which a result is considered "poor" for triggering analysis when `DEBUG_POOR_RANKINGS` is true.
*   `MAX_RANK_TO_CHECK`: (Optional, `evaluate`/`optimize` modes, default: 100) How deep in the search results to look for the target document.
*   _(Other variables like `VESPA_NAMESPACE`, `VESPA_CLUSTER_NAME`, `DELAY_MS`, `MAX_*_RETRIES` exist but are less commonly changed)._

## Usage Examples

**1. Generate Mode:**

Create a dataset with 20 samples, generating 5 queries per document using `qwen3:30b-a3b`, with high concurrency.

```bash
export OLLAMA_MODEL='qwen3:30b-a3b'
export EVAL_MODE='generate'
export EVALUATION_USER_EMAIL='your.email@example.com'
export NUM_SAMPLES=20
export QUERIES_PER_DOC=5
export GENERATE_CONCURRENCY=10 # Optional

bun run scripts/evaluateSearchQualityLLM.ts
```
_(A new `.json` file will be created in `server/eval-results/search-quality-llm/`)_

**2. Evaluate Mode:**

Evaluate search quality using a previously generated dataset, with a specific alpha and enabling poor ranking analysis.

```bash
export EVAL_MODE='evaluate'
export EVALUATION_USER_EMAIL='your.email@example.com'
export EVAL_DATASET_PATH='server/eval-results/search-quality-llm/evaluation_dataset_qwen3.30b-a3b_YYYY-MM-DDTHH-MM-SS-mmmZ.json' # Replace with your actual dataset file path
export DEFAULT_ALPHA=0.6 # Optional: Evaluate with alpha=0.6
export DEBUG_POOR_RANKINGS=true # Optional: Enable LLM analysis for poor results
export GENERATE_CONCURRENCY=8 # Optional: Evaluate queries in parallel

bun run scripts/evaluateSearchQualityLLM.ts
```
_(Outputs results and saves summary/detail files in `server/eval-results/search-quality-llm/`)_

**3. Optimize Mode:**

Find the best hybrid search alpha value using a previously generated dataset.

```bash
export EVAL_MODE='optimize'
export EVALUATION_USER_EMAIL='your.email@example.com'
export EVAL_DATASET_PATH='server/eval-results/search-quality-llm/evaluation_dataset_qwen3.30b-a3b_YYYY-MM-DDTHH-MM-SS-mmmZ.json' # Replace with your actual dataset file path
export GENERATE_CONCURRENCY=10 # Optional: Speed up evaluation across alphas

bun run scripts/evaluateSearchQualityLLM.ts
```
_(Outputs metrics for each alpha tested and reports the best one.)_ 