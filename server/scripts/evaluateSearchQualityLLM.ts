import { searchVespa, SearchModes, GetRandomDocument } from "@/search/vespa";
import { getLogger } from "@/logger";
import { Subsystem } from "@/types";
import {
  fileSchema,
  mailSchema,
  userSchema,
  eventSchema,
  mailAttachmentSchema,
} from "@/search/types";
import fs from "fs";
import path from "path";
import crypto from "crypto";
import type { Message } from "@aws-sdk/client-bedrock-runtime"; // Required for provider interface
import config from "@/config";
import { getProviderByModel } from "@/ai/provider";
import type { LLMProvider, AIProviders as AIProvidersType, Models } from "@/ai/types"; // Corrected imports
import { getSortedScoredChunks } from "@/search/mappers"; // Import the sorter
import pLimit from 'p-limit'; // Added: Import p-limit

// Configuration
const Logger = getLogger(Subsystem.Eval);
const USER_EMAIL_FOR_SEARCH = process.env.EVALUATION_USER_EMAIL;

// --- Evaluation Mode ---
const EVAL_MODE = process.env.EVAL_MODE; // 'generate' or 'evaluate'
const EVAL_DATASET_PATH = process.env.EVAL_DATASET_PATH; // Required if EVAL_MODE='evaluate'
// --- End Evaluation Mode ---

if (!USER_EMAIL_FOR_SEARCH) {
  Logger.error(
    "Error: Environment variable EVALUATION_USER_EMAIL is not set.",
  );
  Logger.error(
    "This email is required for permission-aware search evaluation.",
  );
  Logger.error(
    "Please set it and run the script again. Example: EVALUATION_USER_EMAIL='user@example.com' bun run ...",
  );
  process.exit(1);
}
Logger.info(`Using email for search evaluation: ${USER_EMAIL_FOR_SEARCH}`);

const NUM_SAMPLES = parseInt(process.env.NUM_SAMPLES || "100", 10); // Samples = Documents to test
const QUERIES_PER_DOC = parseInt(process.env.QUERIES_PER_DOC || "1", 10); // LLM queries per document
const MAX_RANK_TO_CHECK = parseInt(process.env.MAX_RANK_TO_CHECK || "100", 10);
const HITS_PER_PAGE = 10;
const VESPA_NAMESPACE = "namespace"; // TODO: Replace with your actual Vespa namespace
const VESPA_CLUSTER_NAME = "my_content"; // TODO: Replace with your actual cluster name
const DELAY_MS = parseInt(process.env.EVALUATION_DELAY_MS || "10", 10); // Increased default delay for LLM calls
const DEBUG_POOR_RANKINGS = process.env.DEBUG_POOR_RANKINGS === "true";
const POOR_RANK_THRESHOLD = parseInt(
  process.env.POOR_RANK_THRESHOLD || "10",
  10,
);
const OUTPUT_DIR = path.join(
  __dirname,
  "..",
  "eval-results",
  "search-quality-llm", // Separate output directory
);
const OLLAMA_MODEL = config.OllamaModel; // Use the model from the central config
const ADVANCED_MODEL_NAME = process.env.ADVANCED_MODEL_NAME || "qwen3:30b-a3b"; // Model name to trigger advanced prompt
const ADVANCED_PROMPT_CHUNKS = parseInt(process.env.ADVANCED_PROMPT_CHUNKS || "4", 10); // Number of chunks for advanced prompt

const MAX_FETCH_RETRIES_PER_SAMPLE = 5; // Max attempts to find a suitable doc per sample
const MAX_LLM_RETRIES_PER_DOC = 3; // Max attempts to generate queries for a doc
const MAX_VERIFICATION_RETRIES = 3; // Added: Max attempts for verification LLM calls
const MAX_ANALYSIS_RETRIES = 3; // Max attempts for analysis LLM calls
const MAX_BODY_CHARS_FOR_PROMPT = 500; // Max characters of body snippet for standard prompt
const GENERATE_CONCURRENCY = parseInt(process.env.GENERATE_CONCURRENCY || '5', 10); // Added: Concurrency limit

// Helper function to ensure directory exists
function ensureDirectoryExists(dirPath: string) {
  if (!fs.existsSync(dirPath)) {
    Logger.info(`Creating output directory: ${dirPath}`);
    fs.mkdirSync(dirPath, { recursive: true });
  }
}

// Mapping from sddocname to relevant fields
const schemaFieldMap: Record<
  string,
  { idField: string; titleField: string; bodyField?: string; metadataFields?: string[] } // Added metadataFields
> = {
  [fileSchema]: {
    idField: "docId",
    titleField: "title",
    bodyField: "chunks",
    metadataFields: ["owner", "createdAt", "updatedAt", "mimeType"],
  },
  [mailSchema]: {
    idField: "docId",
    titleField: "subject",
    bodyField: "chunks",
    metadataFields: ["from", "to", "timestamp", "labels"],
  },
  [userSchema]: {
    idField: "docId",
    titleField: "name",
    // No body field typically for users
    metadataFields: ["email", "orgJobTitle", "orgDepartment", "creationTime"],
  },
  [eventSchema]: {
    idField: "docId",
    titleField: "name",
    bodyField: "description",
    metadataFields: ["location", "startTime", "endTime", "organizer", "attendees"],
  },
  [mailAttachmentSchema]: {
    idField: "docId",
    titleField: "filename",
    bodyField: "chunks",
    metadataFields: ["fileType", "timestamp", "partId"], // Added partId here as potentially useful metadata
  },
};

// Types
interface VespaFields extends Record<string, any> {
  sddocname?: string;
}

interface Document {
  id: string; // User-facing ID (e.g., email message ID, file ID)
  vespaId: string; // Full Vespa document ID (e.g., id:namespace:schema::user_id)
  fields: VespaFields;
}

interface DebugFailureInfo {
  docId: string; // User-facing ID
  vespaId: string; // Full Vespa ID
  query: string;
  llmModel: string; // Model used for query generation
  foundAtRank: number | null;
  debugInfo: {
    query: string;
    docIdToFind: string; // User-facing ID
    vespaIdToFind: string; // Full Vespa ID
    topResults: Array<{ // Only top 10 results
      rank: number;
      schema: string;
      title: string;
      docId: string; // User-facing ID
      vespaId: string; // Full Vespa ID
    }>;
  };
}

interface Metrics {
  mrr: number;
  ndcgAt10: number;
  successAt1: number;
  successAt3: number;
  successAt5: number;
  successAt10: number;
  successRate: number;
  rankDistribution: Record<string, number>;
}

interface EvaluationDatasetItem {
  docId: string;
  vespaId: string;
  schema: string;
  title: string;
  llmModel: string;
  queries: string[];
  sourceDocContext: {
    docId: string;
    vespaId: string;
    schema: string;
    title: string;
    metadataSnippet: Record<string, any>;
    topNChunks?: string[];
  }
}

// DebugAnalysisEntry now only used internally within evaluateFromDataset
interface DebugAnalysisEntry {
    sourceDocContext: EvaluationDatasetItem['sourceDocContext'];
    evaluationFailure: DebugFailureInfo;
}

// Copied from analyzePoorSearchRankingsLLM.ts
interface LLMAnalysisResult {
    assessment: 'GOOD_QUERY_RANKING_ISSUE' | 'BAD_QUERY_IRRELEVANT' | 'BAD_QUERY_TOO_GENERAL' | 'BAD_QUERY_TOO_SPECIFIC' | 'UNCLEAR' | 'ERROR';
    reasoning: string;
    confidence?: number;
}

// Structure to hold analysis results alongside the entry
interface DetailedAnalysisResult {
    entry: DebugAnalysisEntry;
    analysis: LLMAnalysisResult;
}

// Define the structure of evaluation results explicitly
interface EvaluationResult {
  docId: string;
  vespaId: string;
  schema: string;
  title: string;
  rank: number | null;
  query: string;
  llmModel: string;
}

// Added: Type for LLM Query Verification
interface LLMVerificationResult {
    assessment: 'GOOD_QUERIES' | 'BAD_QUERIES_IRRELEVANT' | 'BAD_QUERIES_GENERIC' | 'BAD_QUERIES_OBSCURE' | 'ERROR';
    reasoning: string;
}

// Fetch random document from Vespa
async function getRandomDocument(): Promise<Document | null> {
  const schemas = Object.keys(schemaFieldMap);
  const targetSchema = schemas[Math.floor(Math.random() * schemas.length)];
  Logger.debug(`Fetching random document from schema: ${targetSchema}...`);

  try {
    const doc = await GetRandomDocument(
      VESPA_NAMESPACE,
      targetSchema,
      VESPA_CLUSTER_NAME,
    );

    // Basic validation of the response structure
    if (!doc || !doc.id || !doc.fields) {
      Logger.warn(
        { responseData: doc, schema: targetSchema },
        "Received invalid data structure from GetRandomDocument",
      );
      return null;
    }

    const vespaId = doc.id; // e.g., "id:namespace:schema::user_provided_id"
    const idParts = vespaId.split("::");
    // Infer user-facing ID from Vespa ID or fallback to docId field
    const userFacingId = idParts[1] || doc.fields.docId || vespaId;
    const sddocname = doc.id.split(":")[2] || targetSchema; // Infer schema from Vespa ID

    // More specific validation based on schema mapping
    const fieldMapping = schemaFieldMap[sddocname];
    if (!fieldMapping) {
      Logger.warn(
        { vespaId, inferredSchema: sddocname },
        "Cannot process document: Schema not found in schemaFieldMap",
      );
      return null;
    }

    const idField = fieldMapping.idField;
    const titleField = fieldMapping.titleField;
    const docIdFromFields = doc.fields[idField];
    const titleFromFields = doc.fields[titleField];

    if (
      docIdFromFields === undefined ||
      docIdFromFields === null ||
      typeof titleFromFields !== "string" ||
      titleFromFields.trim() === ""
    ) {
      Logger.warn(
        {
          vespaId,
          sddocname,
          idField,
          titleField,
          docIdValue: docIdFromFields,
          titleValue: titleFromFields,
        },
        `Document missing required ID ('${idField}') or Title ('${titleField}') field, or title is empty. Skipping.`,
      );
      return null;
    }

    // Ensure userFacingId consistency if possible
    if (String(docIdFromFields) !== userFacingId && idParts[1]) {
      Logger.debug(
        { vespaId, userFacingId, idField, docIdFromFields },
        `User-facing ID from Vespa ID ('${userFacingId}') differs from field '${idField}' ('${docIdFromFields}'). Using ID from field.`,
      );
    }

    // Return the structured document
    return {
      id: userFacingId, // Use the ID inferred from Vespa ID structure or field map
      vespaId: vespaId,
      fields: { ...(doc.fields as VespaFields), sddocname },
    };
  } catch (error) {
    Logger.error(
      {
        error: error instanceof Error ? error.message : String(error),
        stack: error instanceof Error ? error.stack : undefined,
        schema: targetSchema,
      },
      "Failed to get random document",
    );
    return null;
  }
}

// Generate search queries using Ollama Provider - MODIFIED
async function generateSearchQueries(doc: Document): Promise<{ queries: string[], sourceDocContext?: EvaluationDatasetItem['sourceDocContext'] | null }> { // Return queries AND context
  const fields = doc.fields as VespaFields;
  const sddocname = fields.sddocname;

  if (!sddocname || !schemaFieldMap[sddocname]) {
    Logger.warn({ docId: doc.id, sddocname }, "Cannot generate query: Unknown schema");
    return { queries: [], sourceDocContext: null };
  }

  const { titleField, bodyField, metadataFields } = schemaFieldMap[sddocname];
  const title = fields[titleField] as string | undefined;

  if (!title || title.trim() === "") {
    Logger.warn({ docId: doc.id, sddocname, titleField }, `Cannot generate query: Missing or empty title field ('${titleField}')`);
    return { queries: [], sourceDocContext: null };
  }

  // Prepare metadata snippet for both prompt and debug context
  const metadataSnippet: Record<string, any> = {};
  if (metadataFields) {
    metadataFields.forEach(key => {
      if (fields[key] !== undefined && fields[key] !== null) {
          // Basic cleanup/shortening for potentially long metadata like attendees
          if (Array.isArray(fields[key]) && fields[key].length > 5) {
              metadataSnippet[key] = fields[key].slice(0, 5).join(', ') + '...';
          } else if (Array.isArray(fields[key])) {
               metadataSnippet[key] = fields[key].join(', ');
          }
           else {
              metadataSnippet[key] = fields[key];
          }
      }
    });
  }

  // Prepare top N chunks for both prompt and debug context
  let topChunksText = "No content chunks available.";
  let chunks_summary : string[] = [];
  if (fields.matchfeatures && Array.isArray(fields.chunks_summary)) {
     chunks_summary = getSortedScoredChunks(fields.matchfeatures, fields.chunks_summary).map(c => c.chunk);
  } else if (Array.isArray(fields.chunks)) {
      chunks_summary = fields.chunks; // Use as is if no matchfeatures
  } else if(sddocname == "event") {
      chunks_summary = [fields.description];
  } else if (sddocname == "user") {
      chunks_summary = [fields.name + " " + fields.email]
  }
  // Use chunks if available for the prompt, limit to a reasonable number if too many
  const chunksForPrompt = chunks_summary.slice(0, 10); // Limit to 10 chunks for prompt clarity

  if (chunksForPrompt.length > 0) {
      topChunksText = chunksForPrompt
          .map((chunk, index) => `--- Chunk ${index + 1} ---
${chunk}`)
          .join('\n');
  }

  if(!chunks_summary.length){
      return {queries: [], sourceDocContext: null}
  }

  // --- NEW SYSTEM AND USER PROMPT ---
  const systemPrompt = `You are an expert search user trying to find a specific document. Based ONLY on the provided document context below, generate ${QUERIES_PER_DOC} diverse, realistic search queries that a user might type to find this exact document.`;

  const sourceDocContextForPrompt = {
    //   docId: doc.id,
    //   vespaId: doc.vespaId,
    //   schema: sddocname,
      title: title,
    //   metadataSnippet,
      // Include chunks in the prompt context if available
      body: chunksForPrompt.length > 0 ? chunksForPrompt : undefined,
  };

  const userPrompt = `**Instructions:**
Generate ${QUERIES_PER_DOC} realistic search queries a user might type to find the document described in the Context below.

1.  **Focus:** Queries should focus on keywords, concepts, or natural language questions related to the document's title, metadata, or content chunks.
2.  **Realism:** Queries should resemble common search engine inputs. Avoid overly specific phrases unlikely to be searched.
3.  **Conciseness:** Aim for queries that are typically 3 to 7 words long.
4.  **Context:** Use ONLY information present in the \`Document Context\` below. Do NOT invent details.
5.  **Body:** Use the \`Body\` field in the \`Document Context\` below to generate queries as well
6.  **Format:** Output ONLY a valid JSON array string containing exactly ${QUERIES_PER_DOC} query strings.
    * Example of desired JSON array output:
      ["weekly securities statement", "invoice pdf", "flight to bangkok", "ethical hacking course", "report from september 20", "digital ocean invoice"]
    * DO NOT include any other text, explanations, reasoning, or formatting outside the single JSON array string.

**Document Context:**
\`\`\`json
${JSON.stringify(sourceDocContextForPrompt, null, 2)}
\`\`\`

/no_think
`;
  // --- END NEW SYSTEM AND USER PROMPT ---


  // Prepare messages for the Ollama provider
  const messages: Message[] = [
    { role: "user", content: [{ text: userPrompt }] },
  ];

  Logger.debug({ docId: doc.id, model: OLLAMA_MODEL }, "Sending request via OllamaProvider");

  try {
     // --- Use central Provider getter ---
    const ollamaProvider = getProviderByModel(OLLAMA_MODEL as Models) as LLMProvider | undefined;

    if (!ollamaProvider) {
        Logger.error( { configuredModel: OLLAMA_MODEL }, "Configuration Error: Ollama provider could not be initialized or found." );
        return { queries: [], sourceDocContext: null };
    }

    const response = await ollamaProvider.converse(messages, {
        modelId: OLLAMA_MODEL as Models,
        systemPrompt: systemPrompt,
        temperature: 0.8, // Increased temperature for more diverse queries
        stream: false,
    });

    // --- Response Parsing (MODIFIED for newline-separated queries) ---
    if (!response || !response.text || response.text.trim() === "") {
      Logger.warn({ docId: doc.id, responseData: response }, "Received empty or invalid response from OllamaProvider");
      return { queries: [], sourceDocContext: null };
    }

    Logger.trace({ docId: doc.id, responseText: response.text }, "Raw response from OllamaProvider");

    let generatedQueries: string[] = [];
    try {
        // --- Clean the response text first (remove potential ```json wrappers) ---
        let cleanedResponseText = response.text.trim();
        // Remove potential markdown code fences
        cleanedResponseText = cleanedResponseText.replace(/^```json\s*/, '').replace(/\s*```$/, '');
        // Remove potential leading/trailing non-JSON characters if any remain (be careful not to strip actual JSON)
        const startIndex = cleanedResponseText.indexOf('[');
        const endIndex = cleanedResponseText.lastIndexOf(']');
        if (startIndex === -1 || endIndex === -1 || endIndex < startIndex) {
             throw new Error("Could not find JSON array start/end brackets in the cleaned response.");
        }
        const jsonString = cleanedResponseText.substring(startIndex, endIndex + 1);
        // --- End Cleaning ---

        const parsedJson = JSON.parse(jsonString);

        if (!Array.isArray(parsedJson)) {
            throw new Error(`LLM response was not a JSON array. Got: ${typeof parsedJson}`);
        }

        // Validate that elements are strings
        if (!parsedJson.every(item => typeof item === 'string')) {
             throw new Error(`Not all elements in the parsed JSON array were strings.`);
        }

        generatedQueries = parsedJson;

        // Optional: Check if the count matches (can be tricky if LLM doesn't always obey)
        if (generatedQueries.length !== QUERIES_PER_DOC) {
            Logger.warn({ docId: doc.id, expected: QUERIES_PER_DOC, generated: generatedQueries.length }, `LLM generated ${generatedQueries.length} queries, but ${QUERIES_PER_DOC} were requested.`);
            // Decide if you want to proceed with the incorrect count or return null
            // For now, we'll proceed but log the warning.
             // If you want to enforce the count strictly, you could uncomment the next line:
             // throw new Error(`LLM did not generate the exact number of queries requested (${QUERIES_PER_DOC}). Got ${generatedQueries.length}.`);
        }

        // Filter out empty strings just in case
        generatedQueries = generatedQueries.filter(q => q.trim().length > 0);

        if (generatedQueries.length === 0) {
            Logger.warn({ docId: doc.id, rawResponse: response.text }, "LLM generated no valid query strings in the JSON array.");
            return { queries: [], sourceDocContext: null };
        }

    } catch (parseError: any) {
        Logger.error({ docId: doc.id, error: parseError.message, rawResponse: response.text }, "Failed to parse JSON array response from OllamaProvider");
        return { queries: [], sourceDocContext: null };
    }
    // --- End Response Parsing ---

    Logger.debug({ docId: doc.id, queries: generatedQueries }, "LLM generated queries successfully via Provider");
    const sourceDocContext: EvaluationDatasetItem['sourceDocContext'] = {
      docId: doc.id,
      vespaId: doc.vespaId,
      schema: sddocname,
      title: title,
      metadataSnippet,
      // Store all chunks in the sourceDocContext for potential analysis
      topNChunks: chunks_summary.length > 0 ? chunks_summary : undefined,
    };
    return { queries: generatedQueries, sourceDocContext };

  } catch (error: any) {
    Logger.error({ docId: doc.id, model: OLLAMA_MODEL, error: error.message || String(error) }, "Failed to generate queries using OllamaProvider");
    return { queries: [], sourceDocContext: null };
  }
}

// Find the rank of a specific document for a given query
async function findDocumentRank(
  docIdToFind: string, // User-facing ID
  vespaIdToFind: string, // Full Vespa ID
  query: string,
  alpha: number // Added: Alpha value for hybrid search
): Promise<{ rank: number | null; debugPayload: DebugFailureInfo | null }> {
  Logger.debug({ docIdToFind, vespaIdToFind, query, alpha }, "findDocumentRank called");

  let rank: number | null = null;
  let offset = 0;
  let collectedDebugInfo: DebugFailureInfo | null = null; // Store full debug info here if debugging is enabled

  try {
    while (offset < MAX_RANK_TO_CHECK) {
      const searchOptions: Record<string, any> = {
        limit: HITS_PER_PAGE,
        offset: offset,
        rankProfile: SearchModes.NativeRank, // Or your custom hybrid rank profile name
        // Corrected: Pass alpha using the key expected by searchVespa in vespa.ts
        alpha,
        ...(DEBUG_POOR_RANKINGS ? { tracelevel: 5 } : {}), // Enable trace only if debugging
      };

      // Assuming searchVespa takes the query string directly
      const response = await searchVespa(
        query,
        USER_EMAIL_FOR_SEARCH!,
        null, // filters
        null, // groupBy
        searchOptions,
      );

      const hits = response.root.children || [];
      const totalCount = response.root.fields?.totalCount || 0;

      // --- Debug Info Collection (if enabled) ---
      if (DEBUG_POOR_RANKINGS) {
          // Collect debug info for each page if debugging is enabled
          // This ensures we have info even if the poor rank is on a later page or not found
          if (!collectedDebugInfo) { // Initialize only on the first page or if an error cleared it
               collectedDebugInfo = { // Create the basic structure
                    docId: docIdToFind,
                    vespaId: vespaIdToFind,
                    query: query,
                    llmModel: OLLAMA_MODEL,
                    foundAtRank: null, // Placeholder, updated if found
                    debugInfo: {
                        query: query,
                        docIdToFind: docIdToFind,
                        vespaIdToFind: vespaIdToFind,
                        topResults: [], // Will populate with hits from current page
                    },
                };
          }

          // Append hits from the current page to topResults (up to 10 per page)
          const pageTopResults = hits.slice(0, 10).map((hit: any, index: number) => {
              const currentRank = offset + index + 1;
              const hitVespaId = hit.id || "unknown_vespa_id";
              const fields = hit.fields as VespaFields;
              const sddocname = fields?.sddocname || hitVespaId.split(":")[2] || "unknown";
              const fieldMapping = schemaFieldMap[sddocname];
              let hitUserFacingId = "unknown_user_id";
              let hitTitle = "unknown_title";

              if (fieldMapping) {
                 hitUserFacingId = fields?.[fieldMapping.idField] ? String(fields[fieldMapping.idField]) : (hitVespaId.split("::")[1] || hitVespaId);
                 hitTitle = fields?.[fieldMapping.titleField] ? String(fields[fieldMapping.titleField]) : "unknown_title";
              } else {
                 // Fallback if schema unknown
                 hitUserFacingId = hitVespaId.split("::")[1] || hitVespaId;
              }

              return {
                rank: currentRank,
                schema: sddocname,
                title: hitTitle,
                docId: hitUserFacingId,
                vespaId: hitVespaId,
              };
          });
           // Only keep up to 10 results total in topResults to match DebugFailureInfo structure expectation
          collectedDebugInfo.debugInfo.topResults = collectedDebugInfo.debugInfo.topResults.concat(pageTopResults).slice(0, 10);

          // Note: Trace is still omitted here for brevity, but could be added if needed from response.root.trace
      }
      // --- End Debug Info Collection ---

      if (hits.length === 0) {
        break; // No more hits from Vespa
      }

      // Check hits on the current page
      for (let i = 0; i < hits.length; i++) {
        const hit = hits[i];
        const currentRank = offset + i + 1;
        const hitVespaId = hit.id; // Get the full Vespa ID from the hit

        Logger.trace( { rank: currentRank, hitVespaId, vespaIdToFind }, "Comparing hit Vespa ID");

        // Compare using the full Vespa ID for accuracy
        if (hitVespaId && hitVespaId === vespaIdToFind) {
          rank = currentRank;
          Logger.debug( `---> Match found at rank ${rank}! Vespa ID: ${vespaIdToFind}`);
          // Update rank in collected debug info if it exists
          if (collectedDebugInfo) {
            collectedDebugInfo.foundAtRank = rank;
          }
          break; // Exit inner loop once found
        }
      }

      if (rank !== null) {
        break; // Exit outer loop once found
      }

      offset += HITS_PER_PAGE;

      // Stop if we've checked all available documents reported by Vespa or reached MAX_RANK_TO_CHECK limit
       if (totalCount > 0 && offset >= totalCount && totalCount <= MAX_RANK_TO_CHECK) {
           Logger.debug({docIdToFind, totalCount, offset}, `Stopped pagination: Checked all ${totalCount} available documents reported by Vespa.`);
           break;
       } else if (offset >= MAX_RANK_TO_CHECK && totalCount > MAX_RANK_TO_CHECK) {
            Logger.debug({docIdToFind, totalCount, offset, MAX_RANK_TO_CHECK}, `Stopped pagination: Reached MAX_RANK_TO_CHECK limit, but more total hits available.`);
            break;
       } else if (totalCount === 0 && offset > 0) {
           Logger.debug({docIdToFind, offset}, "Stopped pagination: No total count reported and no hits returned on current page.");
            break;
       }

    } // End while loop

    // --- Save Individual Debug File REMOVED ---

  } catch (error) {
    Logger.error(
      {
        error: error instanceof Error ? error.message : String(error),
      },
      "Failed to search for document rank due to error",
    );
    // If an error occurred, ensure basic debug info exists if debugging is on
    if (DEBUG_POOR_RANKINGS && !collectedDebugInfo) {
         collectedDebugInfo = {
              docId: docIdToFind,
              vespaId: vespaIdToFind,
              query: query,
              llmModel: OLLAMA_MODEL,
              foundAtRank: null, // Indicate error/unknown rank
              debugInfo: { // Basic info available
                  query: query,
                  docIdToFind: docIdToFind,
                  vespaIdToFind: vespaIdToFind,
                  topResults: [], // No results due to error
              },
          };
    }
    // Return null rank and the potentially partial debug info
    return { rank: null, debugPayload: collectedDebugInfo };
  }

  // Return rank and the debug payload IF debugging is enabled and rank is poor/null
  // The full list of collectedDebugInfo is always passed back if debugging is on.
  return {
    rank,
    debugPayload:
      DEBUG_POOR_RANKINGS && (rank === null || (rank !== null && rank > POOR_RANK_THRESHOLD))
        ? collectedDebugInfo // Return the collected debug info
        : null, // Return null if not debugging or rank is good
  };
}

// Calculate evaluation metrics - MODIFIED to accept EvaluationResult[]
function calculateMetrics(results: EvaluationResult[]): Metrics {
  const numResults = results.length;
  if (numResults === 0) {
    // Return zero metrics if no results to process
    return {
      mrr: 0,
      ndcgAt10: 0,
      successAt1: 0,
      successAt3: 0,
      successAt5: 0,
      successAt10: 0,
      successRate: 0,
      rankDistribution: {
        "1": 0, "2-3": 0, "4-5": 0, "6-10": 0, "11-20": 0,
        "21-50": 0, "51-100": 0, "not_found": 0,
      },
    };
  }

  const reciprocalRanks: number[] = [];
  const ndcgAt10Scores: number[] = []; // Added: Array to store individual NDCG@10 scores
  const idealDcgAt10 = 1 / Math.log2(1 + 1); // IDCG for a single relevant item at rank 1
  const rankDistribution: Record<string, number> = {
    "1": 0, "2-3": 0, "4-5": 0, "6-10": 0, "11-20": 0,
    "21-50": 0, "51-100": 0, "not_found": 0,
  };
  let successAt1Count = 0;
  let successAt3Count = 0;
  let successAt5Count = 0;
  let successAt10Count = 0;
  let successRateCount = 0; // Within MAX_RANK_TO_CHECK

  results.forEach((result) => {
    const rank = result.rank;
    let currentNdcgAt10 = 0; // Added: Variable for current query's NDCG

    if (rank !== null && rank <= MAX_RANK_TO_CHECK) {
      reciprocalRanks.push(1 / rank);
      successRateCount++;
      if (rank <= 10) {
        const dcg = 1 / Math.log2(rank + 1); // Calculate DCG for this query
        currentNdcgAt10 = idealDcgAt10 > 0 ? dcg / idealDcgAt10 : 0; // Calculate NDCG for this query
        successAt10Count++;
         if (rank <= 5) {
            successAt5Count++;
            if (rank <= 3) {
                successAt3Count++;
                if (rank === 1) {
                    successAt1Count++;
                }
            }
         }
      }
      if (rank === 1) rankDistribution["1"]++;
      else if (rank <= 3) rankDistribution["2-3"]++;
      else if (rank <= 5) rankDistribution["4-5"]++;
      else if (rank <= 10) rankDistribution["6-10"]++;
      else if (rank <= 20) rankDistribution["11-20"]++;
      else if (rank <= 50) rankDistribution["21-50"]++;
      else rankDistribution["51-100"]++;
    } else {
      // Do not add to reciprocalRanks or ndcgAt10Scores if not found
      rankDistribution["not_found"]++;
    }
    // Only push scores for found items to the temporary arrays for averaging
    if (rank !== null && rank <= MAX_RANK_TO_CHECK) {
        ndcgAt10Scores.push(currentNdcgAt10);
    }
  });

  const calculateMean = (arr: number[]): number => {
    if (arr.length === 0) return 0;
    const sum = arr.reduce((acc, val) => acc + val, 0);
    return sum / arr.length;
  };

  // Filter out not found cases before calculating mean for MRR and NDCG@10
  const foundResults = results.filter(result => result.rank !== null && result.rank <= MAX_RANK_TO_CHECK);
  const mrr = calculateMean(foundResults.map(result => 1 / result.rank!)); // Calculate mean of reciprocal ranks for found items
  const ndcgAt10 = calculateMean(foundResults.map(result => {
      // Recalculate NDCG for found items if needed, or use the stored currentNdcgAt10
      // For simplicity and consistency with original logic, let's use the stored one.
      // We need to find the corresponding stored NDCG score for this result.
      // A better approach might be to store objects {rank: number, ndcg: number} initially.
      // For now, let's recalculate based on rank for found items.
      const rank = result.rank!;
      const dcg = 1 / Math.log2(rank + 1);
      return idealDcgAt10 > 0 ? dcg / idealDcgAt10 : 0;
  }));


  return {
    mrr: mrr,
    ndcgAt10: ndcgAt10,
    successAt1: successAt1Count / numResults,
    successAt3: successAt3Count / numResults,
    successAt5: successAt5Count / numResults,
    successAt10: successAt10Count / numResults,
    successRate: successRateCount / numResults,
    rankDistribution,
  };
}

// --- Analysis Function (Adapted from analyzePoorSearchRankingsLLM.ts) ---
async function analyzePoorRankingCase(entry: DebugAnalysisEntry, llmProvider: LLMProvider): Promise<LLMAnalysisResult> {
    // Use OLLAMA_MODEL from config for analysis model
    const analysisModel = OLLAMA_MODEL as Models;
    const { sourceDocContext, evaluationFailure } = entry;
    const { query, foundAtRank, debugInfo } = evaluationFailure; // Already checked for null before calling this
    const { topResults } = debugInfo || {}; // debugInfo might still be minimal if search errored

    Logger.debug({ docId: sourceDocContext.docId, query }, "Analyzing poor ranking case...");

    const sourceContextString = `Source Document:\n- Title: "${sourceDocContext.title}"\n- Schema: ${sourceDocContext.schema}\n- Metadata: ${JSON.stringify(sourceDocContext.metadataSnippet)}\n${sourceDocContext.topNChunks ? `- Relevant Chunks:\n${sourceDocContext.topNChunks.map((c, i) => `  - Chunk ${i + 1}: ${c.substring(0, 150)}...`).join('\n')}` : '- No chunks available'}\n`;

    const searchResultsString = `Search Results for Query "${query}":\n- Target document was found at rank: ${foundAtRank ?? 'Not Found'}\n- Top 10 results:\n${topResults?.map(r => `  - Rank ${r.rank}: "${r.title}" (Schema: ${r.schema}, DocID: ${r.docId}, VespaID: ${r.vespaId})`).join('\n') || 'N/A'}\n`;

    const systemPrompt = `You are an expert search quality analyst. Your task is to analyze why a specific search query performed poorly for finding a target document, given the document's content, the query, and the search results. Provide a concise analysis and categorization.`;
    const userPrompt = `Please analyze the following search quality failure case:\n\n${sourceContextString}\n\nGenerated Query (that performed poorly):\n"${query}"\n\n${searchResultsString}\n\nAnalysis Task:\n1. Evaluate the query: Is "${query}" a reasonable and relevant query someone might use to find the Source Document described above? Consider its specificity and keywords.\n2. Analyze the search results: Given the query and the source document, are the top search results relevant? Why might the target document have ranked poorly (at ${foundAtRank ?? 'Not Found'}) or not appeared?\n3. Categorize the failure: Based on your analysis, choose the *best* fitting category from the list below.\n\nCategories:\n- GOOD_QUERY_RANKING_ISSUE: The query was reasonable and relevant, but the search engine failed to rank the target document highly.\n- BAD_QUERY_IRRELEVANT: The query was not relevant to the source document's content.\n- BAD_QUERY_TOO_GENERAL: The query was relevant but too broad, likely matching many other documents.\n- BAD_QUERY_TOO_SPECIFIC: The query was too specific or used terms unlikely to be searched, even if related to the document.\n- UNCLEAR: Not enough information to determine the cause of the poor ranking.\n\nOutput Format:\nReturn *only* a valid JSON object with the keys "assessment" (containing the chosen category string) and "reasoning" (a brief explanation for your assessment, 1-2 sentences). Example:\n{\n  "assessment": "GOOD_QUERY_RANKING_ISSUE",\n  "reasoning": "The query uses relevant keywords from the title and chunks. The target document should have ranked higher given the query."\n}\n\nDO NOT include any other text, explanations, or formatting outside the JSON object.`;

    let lastError: Error | null = null;
    for (let attempt = 1; attempt <= MAX_ANALYSIS_RETRIES; attempt++) {
        try {
            Logger.debug({ docId: sourceDocContext.docId, query, attempt }, `LLM Analysis Attempt ${attempt}/${MAX_ANALYSIS_RETRIES}`);
            const response = await llmProvider.converse(
                [{ role: "user", content: [{ text: userPrompt }] }],
                {
                    modelId: analysisModel,
                    systemPrompt: systemPrompt,
                    temperature: 0.3,
                    stream: false,
                }
            );

            if (!response || !response.text) throw new Error("LLM response was empty.");
            Logger.trace({ docId: sourceDocContext.docId, query, attempt, responseText: response.text }, "Raw LLM analysis response");

            let cleanedResponseText = response.text.replace(/<think>[\s\S]*?<\/think>/g, '').trim();
            cleanedResponseText = cleanedResponseText.replace(/^```json\s*/, '').replace(/\s*```$/g, '').trim();
            
            // --- More Robust JSON Object Extraction ---
            const startIndex = cleanedResponseText.indexOf('{');
            const endIndex = cleanedResponseText.lastIndexOf('}');
            if (startIndex === -1 || endIndex === -1 || endIndex < startIndex) {
                throw new Error("Could not find JSON object start/end braces in the cleaned response.");
            }
            const jsonString = cleanedResponseText.substring(startIndex, endIndex + 1);
            // --- End Robust Extraction ---

            const parsedJson = JSON.parse(jsonString); // Parse the extracted string

            if (typeof parsedJson.assessment !== 'string' || typeof parsedJson.reasoning !== 'string') {
                throw new Error("Parsed JSON does not match the expected LLMAnalysisResult structure.");
            }
            const validAssessments: LLMAnalysisResult['assessment'][] = [
                'GOOD_QUERY_RANKING_ISSUE', 'BAD_QUERY_IRRELEVANT', 'BAD_QUERY_TOO_GENERAL',
                'BAD_QUERY_TOO_SPECIFIC', 'UNCLEAR'
            ];
            if (!validAssessments.includes(parsedJson.assessment)) {
                 Logger.warn({ parsedAssessment: parsedJson.assessment }, "LLM returned an unexpected assessment category.");
            }

            Logger.debug({ docId: sourceDocContext.docId, query, assessment: parsedJson.assessment }, "LLM analysis successful.");
            return parsedJson as LLMAnalysisResult;

        } catch (error: any) {
            lastError = error;
            Logger.warn({ docId: sourceDocContext.docId, query, attempt, error: error?.message }, `LLM analysis attempt ${attempt} failed. Retrying after delay...`);
            await new Promise(resolve => setTimeout(resolve, DELAY_MS * attempt));
        }
    }

    Logger.error({ docId: sourceDocContext.docId, query, error: lastError?.message }, `LLM analysis failed after ${MAX_ANALYSIS_RETRIES} attempts.`);
    return { assessment: 'ERROR', reasoning: `Failed after ${MAX_ANALYSIS_RETRIES} attempts: ${lastError?.message || 'Unknown error'}` };
}
// --- End Analysis Function ---

// Generate the summary report
function generateSummaryReport(
    metrics: Metrics,
    totalQueriesEvaluated: number,
    totalDocsInDataset: number,
    analysisResults: DetailedAnalysisResult[],
    datasetPath?: string
): string {
  const { mrr, ndcgAt10, successAt1, successAt3, successAt5, successAt10, successRate, rankDistribution } = metrics;
  const formatPercent = (value: number) => (value * 100).toFixed(2);

  let report = `
=================================================
 SEARCH QUALITY EVALUATION SUMMARY
=================================================
Generated: ${new Date().toISOString()}
`;
  if (datasetPath) report += `Evaluation Dataset: ${path.basename(datasetPath)}\n`;
  report += `LLM Model Used: ${config.OllamaModel}
`;
  report += `Target Documents in Dataset: ${totalDocsInDataset}
`;
  report += `Total Queries Evaluated From Dataset: ${totalQueriesEvaluated}
`;
  report += `Max Rank Checked per Query: ${MAX_RANK_TO_CHECK}
`;
  report += `Poor Rank Threshold (for Debugging): ${POOR_RANK_THRESHOLD}
`;
  report += `
---------------------
 OVERALL METRICS
---------------------
`;
  report += `Mean Reciprocal Rank (MRR): ${mrr.toFixed(4)}
`;
  report += `NDCG@10: ${ndcgAt10.toFixed(4)}
`; // NOTE: Verify NDCG calculation if needed
  report += `
---------------------
 SUCCESS RATES (Precision@K)
---------------------
`;
  report += `Found @ Rank 1:   ${formatPercent(successAt1)}% (${rankDistribution["1"]} queries)\n`;
  report += `Found @ Rank <= 3:  ${formatPercent(successAt3)}% (${rankDistribution["1"] + rankDistribution["2-3"]} queries)\n`;
  report += `Found @ Rank <= 5:  ${formatPercent(successAt5)}% (${rankDistribution["1"] + rankDistribution["2-3"] + rankDistribution["4-5"]} queries)\n`;
  report += `Found @ Rank <= 10: ${formatPercent(successAt10)}% (${rankDistribution["1"] + rankDistribution["2-3"] + rankDistribution["4-5"] + rankDistribution["6-10"]} queries)\n`;
  report += `Found @ Rank <= ${MAX_RANK_TO_CHECK}: ${formatPercent(successRate)}% (${totalQueriesEvaluated - rankDistribution["not_found"]} queries)\n`;
  report += `
---------------------
 DISTRIBUTION OF RANKS (for ${totalQueriesEvaluated} queries)
---------------------
`;
  const totalFound = totalQueriesEvaluated - rankDistribution["not_found"];
  for (const [bin, count] of Object.entries(rankDistribution)) {
    report += `Rank ${bin.padEnd(10)}: ${String(count).padStart(5)} (${formatPercent(count / totalQueriesEvaluated)}%)\n`;
  }

  // --- Add Analysis Summary Section --- (if DEBUG_POOR_RANKINGS was true)
  if (DEBUG_POOR_RANKINGS && analysisResults.length > 0) {
      report += `
---------------------
 POOR RANKING ANALYSIS SUMMARY (${analysisResults.length} cases analyzed)
---------------------
`;
      const analysisCounts: Record<LLMAnalysisResult['assessment'], number> = {
          'GOOD_QUERY_RANKING_ISSUE': 0,
          'BAD_QUERY_IRRELEVANT': 0,
          'BAD_QUERY_TOO_GENERAL': 0,
          'BAD_QUERY_TOO_SPECIFIC': 0,
          'UNCLEAR': 0,
          'ERROR': 0
      };
      analysisResults.forEach(res => {
          if (res.analysis && res.analysis.assessment) {
              analysisCounts[res.analysis.assessment]++;
          }
      });

      for (const [assessment, count] of Object.entries(analysisCounts)) {
          const percentage = analysisResults.length > 0 ? (count / analysisResults.length * 100).toFixed(1) : '0.0';
          report += `${assessment.padEnd(28)}: ${count} (${percentage}%)\n`;
      }
      report += `\n(See analysis_details_*.json for full details if saved)\n`;
  } else if (DEBUG_POOR_RANKINGS) {
      report += `
---------------------
 POOR RANKING ANALYSIS
---------------------
Debugging enabled, but no poor rankings were analyzed (check thresholds and logs).
`;
  }
  // --- End Analysis Summary Section ---

  report += `
=================================================
 END OF REPORT
=================================================
`;
  return report;
}

// --- Added: LLM Query Verification Function ---
async function verifyGeneratedQueries(
    sourceDocContext: EvaluationDatasetItem['sourceDocContext'],
    generatedQueries: string[],
    llmProvider: LLMProvider
): Promise<LLMVerificationResult> {
    const { title, schema, metadataSnippet, topNChunks } = sourceDocContext;
    const queriesString = generatedQueries.map(q => `\"${q}\"`).join(", ");

    Logger.debug({ docId: sourceDocContext.docId, queries: generatedQueries }, "Verifying generated queries...");

    const sourceContextString = `Source Document:\n- Title: \"${title}\"\n- Schema: ${schema}\n- ${topNChunks ? `- Relevant Chunks:\n${topNChunks.map((c, i) => `  - Chunk ${i + 1}: ${c.substring(0, 150)}...`).join('\n')}` : '- No chunks available'}\n`;

    const systemPrompt = `You are an expert search quality analyst. Your task is to evaluate if a set of generated search queries are relevant, specific, and resemble queries a real user might use to find the given source document.`;
    const userPrompt = `Please evaluate the following search queries intended to find the source document described below:\n\n${sourceContextString}\n\nGenerated Queries:\n[${queriesString}]\n\nVerification Task:\n1. Assess Relevance: Are the queries clearly related to the document's title, or content chunks?\n2. Assess Specificity: Are the queries specific enough to likely find this document, or are they too generic (could match many things) or too obscure (unlikely to be searched)?\n3. Categorize the Quality: Based on your assessment, choose the *best* fitting category from the list below.\n\nCategories:\n- GOOD_QUERIES: The queries are relevant, reasonably specific, and seem like plausible user searches for this document.\n- BAD_QUERIES_IRRELEVANT: Most queries are not relevant to the source document's content.\n- BAD_QUERIES_GENERIC: The queries are relevant but too broad/generic, likely matching many other documents.\n- BAD_QUERIES_OBSCURE: The queries might be relevant but are too specific or use terms/phrases unlikely to be searched by a real user.\n\nOutput Format:\nReturn *only* a valid JSON object with the keys "assessment" (containing the chosen category string) and "reasoning" (a brief explanation for your assessment, 1-2 sentences). Example:\n{\n  "assessment": "GOOD_QUERIES",\n  "reasoning": "Queries use relevant keywords from title and include plausible natural language questions."\n}\n\nDO NOT include any other text, explanations, or formatting outside the JSON object.`;

    let lastError: Error | null = null;
    for (let attempt = 1; attempt <= MAX_VERIFICATION_RETRIES; attempt++) {
        try {
            Logger.debug({ docId: sourceDocContext.docId, attempt }, `LLM Verification Attempt ${attempt}/${MAX_VERIFICATION_RETRIES}`);
            const response = await llmProvider.converse(
                [{ role: "user", content: [{ text: userPrompt }] }],
                {
                    modelId: OLLAMA_MODEL as Models,
                    systemPrompt: systemPrompt,
                    temperature: 0.3,
                    stream: false,
                }
            );

            if (!response || !response.text) throw new Error("LLM response was empty.");
            Logger.trace({ docId: sourceDocContext.docId, attempt, responseText: response.text }, "Raw LLM verification response");

            let cleanedResponseText = response.text.replace(/<think>[\s\S]*?<\/think>/g, '').trim();
            cleanedResponseText = cleanedResponseText.replace(/^```json\s*/, '').replace(/\s*```$/g, '').trim();
            const jsonMatch = cleanedResponseText.match(/(\{.*?\})/s);
            if (!jsonMatch || !jsonMatch[0]) throw new Error("No JSON object found in the cleaned LLM response.");
            const parsedJson = JSON.parse(jsonMatch[0]);

            if (typeof parsedJson.assessment !== 'string' || typeof parsedJson.reasoning !== 'string') {
                throw new Error("Parsed JSON does not match the expected LLMVerificationResult structure.");
            }
            // Optional: Add validation for assessment values if needed

            Logger.debug({ docId: sourceDocContext.docId, assessment: parsedJson.assessment }, "LLM verification successful.");
            return parsedJson as LLMVerificationResult;

        } catch (error: any) {
            lastError = error;
            Logger.warn({ docId: sourceDocContext.docId, attempt, error: error?.message }, `LLM verification attempt ${attempt} failed. Retrying after delay...`);
            await new Promise(resolve => setTimeout(resolve, DELAY_MS * attempt));
        }
    }

    Logger.error({ docId: sourceDocContext.docId, error: lastError?.message }, `LLM verification failed after ${MAX_VERIFICATION_RETRIES} attempts.`);
    return { assessment: 'ERROR', reasoning: `Failed after ${MAX_VERIFICATION_RETRIES} attempts: ${lastError?.message || 'Unknown error'}` };
}
// --- End Verification Function ---

// --- Added: Function to process a single sample (fetch -> generate -> verify) ---
async function processSingleSample(sampleIndex: number, totalSamples: number, queryGenProvider: LLMProvider): Promise<EvaluationDatasetItem | null> {
    Logger.debug(`--- Starting attempt for Verified Sample ${sampleIndex + 1}/${totalSamples} ---`);

    let doc: Document | null = null;
    let foundSuitableDocument = false;

    // 1. Attempt to find a suitable document
    for (let fetchAttempt = 1; fetchAttempt <= MAX_FETCH_RETRIES_PER_SAMPLE; fetchAttempt++) {
        doc = await getRandomDocument();
        if (doc) {
            foundSuitableDocument = true;
            Logger.debug(`[Sample ${sampleIndex + 1}] Found candidate document (ID: ${doc.id})`);
            break;
        }
        await new Promise((resolve) => setTimeout(resolve, DELAY_MS * 2)); // Wait before retry
    }

    if (!foundSuitableDocument || !doc) {
        Logger.warn(`[Sample ${sampleIndex + 1}] Failed to fetch a suitable document after ${MAX_FETCH_RETRIES_PER_SAMPLE} attempts. Skipping this sample attempt.`);
        return null; // Failed to get a document for this sample
    }

    // 2. Attempt to generate queries for the found document
    let generatedResult: { queries: string[], sourceDocContext?: EvaluationDatasetItem['sourceDocContext'] | null } = { queries: [], sourceDocContext: null };
    let generatedQueriesForDoc = false;
    for (let genAttempt = 1; genAttempt <= MAX_LLM_RETRIES_PER_DOC; genAttempt++) {
        generatedResult = await generateSearchQueries(doc);
        if (generatedResult.queries.length >= QUERIES_PER_DOC && generatedResult.sourceDocContext) {
            generatedQueriesForDoc = true;
            break;
        }
        if (generatedResult.queries.length > 0 && genAttempt === MAX_LLM_RETRIES_PER_DOC) {
            Logger.warn(`[Sample ${sampleIndex + 1}] Generated only ${generatedResult.queries.length}/${QUERIES_PER_DOC} queries for Doc ID ${doc.id}, using partial list.`);
            generatedQueriesForDoc = true;
            break;
        }
        Logger.warn(`[Sample ${sampleIndex + 1}] Query Gen Attempt ${genAttempt} failed or insufficient queries for Doc ID ${doc.id}. Retrying...`);
        await new Promise((resolve) => setTimeout(resolve, DELAY_MS * (genAttempt + 1))); // Longer delay for retries
    }

    if (!generatedQueriesForDoc || !generatedResult.sourceDocContext) {
        Logger.warn(`[Sample ${sampleIndex + 1}] Failed to generate sufficient queries or capture context for Doc ID ${doc.id}. Skipping sample.`);
        return null; // Failed generation
    }

    // 3. Verify Generated Queries
    const verificationResult = await verifyGeneratedQueries(generatedResult.sourceDocContext, generatedResult.queries, queryGenProvider);
    if (verificationResult.assessment !== 'GOOD_QUERIES') {
        Logger.warn({ docId: doc.id, assessment: verificationResult.assessment, reason: verificationResult.reasoning }, `[Sample ${sampleIndex + 1}] Generated queries failed verification for Doc ID ${doc.id}. Skipping sample.`);
        return null; // Failed verification
    }

    Logger.info(`[Sample ${sampleIndex + 1}] Generated queries VERIFIED as GOOD for Doc ID ${doc.id}.`);

    // 4. Return verified dataset item
    const verifiedItem: EvaluationDatasetItem = {
        docId: doc.id,
        vespaId: doc.vespaId,
        schema: generatedResult.sourceDocContext.schema,
        title: generatedResult.sourceDocContext.title,
        llmModel: config.OllamaModel,
        queries: generatedResult.queries,
        sourceDocContext: generatedResult.sourceDocContext,
    };

    return verifiedItem;
}
// --- End processSingleSample ---

// --- Generate Dataset Function (MODIFIED for Concurrency) ---
async function generateEvaluationDataset(): Promise<void> {
    Logger.info(`Starting dataset generation for ${NUM_SAMPLES} documents...`);
    Logger.info(`Using Ollama Model from config: ${config.OllamaModel}`);
    Logger.info(`Concurrency Level: ${GENERATE_CONCURRENCY}`);
    Logger.info(`Output directory: ${OUTPUT_DIR}`);

    if (!config.OllamaModel) {
        Logger.error("Error: OLLAMA_MODEL is not configured.");
        process.exit(1);
    }
    const queryGenProvider = getProviderByModel(OLLAMA_MODEL as Models) as LLMProvider | undefined;
    if (!queryGenProvider) {
        Logger.error({ model: OLLAMA_MODEL }, "Failed to initialize LLM provider for query generation/verification.");
        process.exit(1);
    }

    ensureDirectoryExists(OUTPUT_DIR);

    const limit = pLimit(GENERATE_CONCURRENCY);
    const tasks: Promise<EvaluationDatasetItem | null>[] = [];

    Logger.info(`Queueing ${NUM_SAMPLES} sample generation tasks...`);
    for (let i = 0; i < NUM_SAMPLES; i++) {
        // Wrap processSingleSample call in the limiter
        tasks.push(limit(() => processSingleSample(i, NUM_SAMPLES, queryGenProvider)));
    }

    // Wait for all tasks to settle (either resolve or reject)
    const results = await Promise.allSettled(tasks);

    // Filter out failed attempts and collect successful dataset items
    const dataset: EvaluationDatasetItem[] = [];
    results.forEach((result, index) => {
        if (result.status === 'fulfilled' && result.value) {
            dataset.push(result.value);
            Logger.info(`[✓ Sample ${index + 1}/${NUM_SAMPLES}] Successfully generated and verified.`);
        } else {
            // Log rejection reason or null value
            const reason = result.status === 'rejected' ? result.reason : 'Processing failed or returned null';
            Logger.error(`[✗ Sample ${index + 1}/${NUM_SAMPLES}] Failed: ${reason instanceof Error ? reason.message : String(reason)}`);
        }
    });

    const documentsSuccessfullyProcessed = dataset.length;

    Logger.info(`--- Dataset Generation Finished ---`);
    Logger.info(`Successfully generated and verified ${documentsSuccessfullyProcessed} dataset entries out of ${NUM_SAMPLES} attempts.`);

    // Save the dataset file if any entries were successful
    if (dataset.length > 0) {
        const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
        const modelNameForFilename = config.OllamaModel.replace(/:/g, '.');
        const datasetFilename = path.join(OUTPUT_DIR, `evaluation_dataset_${modelNameForFilename}_${timestamp}.json`);
        try {
            fs.writeFileSync(datasetFilename, JSON.stringify(dataset, null, 2));
            Logger.info(`Evaluation dataset (${documentsSuccessfullyProcessed} entries) saved to: ${datasetFilename}`);
        } catch (error) {
            Logger.error({ error }, "Failed to save evaluation dataset file.");
        }
    } else {
        Logger.warn("No documents were successfully processed and verified, dataset file not saved.");
    }
}

// --- Evaluate From Dataset Function (MODIFIED for alpha testing) ---
async function evaluateFromDataset(datasetPath: string, alpha: number): Promise<Metrics | null> { // Added alpha, return Metrics | null
    Logger.info(`Starting evaluation from dataset: ${datasetPath} with alpha=${alpha}`); // Log alpha
    if (!fs.existsSync(datasetPath)) {
        Logger.error(`Dataset file not found: ${datasetPath}`);
        process.exit(1);
    }

    let dataset: EvaluationDatasetItem[];
    try {
        const fileContent = fs.readFileSync(datasetPath, 'utf-8');
        dataset = JSON.parse(fileContent);
        if (!Array.isArray(dataset) || dataset.length === 0) {
            throw new Error("Dataset file is empty or not a valid JSON array.");
        }
        Logger.info(`Loaded ${dataset.length} documents from dataset.`);
    } catch (error: any) {
        Logger.error({ error: error.message }, `Failed to load or parse dataset file: ${datasetPath}`);
        process.exit(1);
    }

    const firstItemModel = dataset[0]?.llmModel || "Unknown";
    Logger.info(`Dataset generated with LLM Model: ${firstItemModel}`);
    Logger.info(`Evaluation using LLM Model: ${OLLAMA_MODEL}`);
    Logger.info(`Output directory: ${OUTPUT_DIR}`);
    if (DEBUG_POOR_RANKINGS) {
        Logger.warn(`Debugging enabled: Poor rankings (rank > ${POOR_RANK_THRESHOLD} or not found) will be analyzed using LLM.`);
    }

    ensureDirectoryExists(OUTPUT_DIR);
    let analysisLLMProvider: LLMProvider | undefined;
    if (DEBUG_POOR_RANKINGS) {
        analysisLLMProvider = getProviderByModel(OLLAMA_MODEL as Models) as LLMProvider | undefined;
        if (!analysisLLMProvider) {
            Logger.error({ model: OLLAMA_MODEL }, "Failed to initialize LLM provider for analysis. Analysis will be skipped.");
        }
    }

    const evaluationResults: EvaluationResult[] = [];
    const analysisResults: DetailedAnalysisResult[] = [];
    let totalQueriesEvaluated = 0;
    const limit = pLimit(GENERATE_CONCURRENCY); // Use p-limit for concurrency control
    const tasks: Promise<void>[] = [];
    const processingResults: Array<{evalResult: EvaluationResult | null, analysisResult: DetailedAnalysisResult | null, error?: any}> = [];

    Logger.info(`Starting parallel evaluation with concurrency: ${GENERATE_CONCURRENCY}`);

    // Flatten the dataset items and queries into individual tasks
    const allQueriesTasks: Array<{ item: EvaluationDatasetItem; query: string; docIndex: number }> = [];
    dataset.forEach((item, docIndex) => {
        if (item.queries && item.queries.length > 0) {
            item.queries.forEach(query => {
                allQueriesTasks.push({ item, query, docIndex });
            });
        } else {
            Logger.warn({ docId: item.docId }, `[Doc ${docIndex + 1}] Skipping document with no queries in dataset.`);
        }
    });

    totalQueriesEvaluated = allQueriesTasks.length;
    Logger.info(`Prepared ${totalQueriesEvaluated} individual query evaluation tasks.`);

    // Define the function to process a single query
    const processQuery = async (taskData: { item: EvaluationDatasetItem; query: string; docIndex: number }): Promise<{evalResult: EvaluationResult, analysisResult: DetailedAnalysisResult | null}> => {
        const { item, query, docIndex } = taskData;
        Logger.info(
            `[Alpha: ${alpha}, Doc ${docIndex + 1}/${dataset.length}] Evaluating Query: "${query.substring(0, 80)}${query.length > 80 ? "..." : ""}" for Doc ID: ${item.docId}`,
        );

        // Pass alpha to findDocumentRank
        const { rank, debugPayload } = await findDocumentRank(item.docId, item.vespaId, query, alpha); 

        const evalResult: EvaluationResult = {
            docId: item.docId,
            vespaId: item.vespaId,
            schema: item.schema,
            title: item.title,
            rank: rank,
            query: query,
            llmModel: item.llmModel,
        };

        let analysisResult: DetailedAnalysisResult | null = null;

        // Perform Analysis if Poor Rank
        if (DEBUG_POOR_RANKINGS && debugPayload && analysisLLMProvider) {
            if (item.sourceDocContext) {
                const analysisEntry: DebugAnalysisEntry = {
                    sourceDocContext: item.sourceDocContext,
                    evaluationFailure: debugPayload,
                };
                Logger.info(`[Doc ${docIndex + 1}] -> Poor Rank (${rank ?? 'Not Found'}) for Query "${query.substring(0, 30)}...". Performing LLM analysis...`);
                const analysis = await analyzePoorRankingCase(analysisEntry, analysisLLMProvider);
                analysisResult = { entry: analysisEntry, analysis };
                Logger.info(`[Doc ${docIndex + 1}] -> Analysis Complete. Assessment: ${analysis.assessment}`);
            } else {
                Logger.warn({ docId: item.docId, query, rank }, "[Doc ${docIndex + 1}] Poor rank detected but sourceDocContext is missing in dataset entry. Cannot perform analysis.");
            }
        } else if (DEBUG_POOR_RANKINGS && (rank === null || (rank !== null && rank > POOR_RANK_THRESHOLD)) && !debugPayload) {
             Logger.warn({docId: item.docId, query, rank}, "[Doc ${docIndex + 1}] Poor rank detected but debug payload was not generated (check findDocumentRank logic). Cannot perform analysis.");
        } else if (DEBUG_POOR_RANKINGS && (rank === null || (rank !== null && rank > POOR_RANK_THRESHOLD)) && !analysisLLMProvider) {
             Logger.warn({docId: item.docId, query, rank, alpha}, "[Alpha: ${alpha}, Doc ${docIndex + 1}] Poor rank detected but analysis LLM provider failed to initialize. Skipping analysis.");
        }

        Logger.info(`[Alpha: ${alpha}, Doc ${docIndex + 1}] -> Query "${query.substring(0, 30)}..." Rank: ${rank ?? "Not Found (or > " + MAX_RANK_TO_CHECK + ")"}`);
        return { evalResult, analysisResult };
    };

    // Queue tasks using p-limit
    allQueriesTasks.forEach(taskData => {
        tasks.push(limit(async () => {
            try {
                const result = await processQuery(taskData);
                processingResults.push(result);
            } catch (error: any) {
                Logger.error({ docId: taskData.item.docId, query: taskData.query, error: error?.message }, "Error during parallel query processing");
                processingResults.push({ evalResult: null, analysisResult: null, error: error }); // Store error indicator
            }
        }));
    });

    // Wait for all tasks to complete
    await Promise.allSettled(tasks);

    // Collect results from processed tasks
    processingResults.forEach(res => {
        if (res.evalResult) {
            evaluationResults.push(res.evalResult);
        }
        if (res.analysisResult) {
            analysisResults.push(res.analysisResult);
        }
        // Errors already logged within the task
    });

    Logger.info(`--- [Alpha: ${alpha}] Parallel Evaluation Finished ---`);
    Logger.info(`Documents Considered: ${dataset.length}`); // Changed log message slightly
    Logger.info(`Total Queries Evaluated: ${totalQueriesEvaluated}`);
    Logger.info(`Successful Evaluations: ${evaluationResults.length}`);
    Logger.info(`Analyses Performed (if enabled): ${analysisResults.length}`);

    if (totalQueriesEvaluated === 0 || evaluationResults.length === 0) { // Check evaluationResults too
        Logger.warn(`[Alpha: ${alpha}] No queries were successfully evaluated from the dataset. Cannot calculate metrics.`);
        return null; // Return null if no results
    }

    const metrics = calculateMetrics(evaluationResults);
    Logger.info(`--- [Alpha: ${alpha}] Final Metrics (from Dataset: ${path.basename(datasetPath)}) ---`);
    Logger.info(`MRR: ${metrics.mrr.toFixed(4)}`);
    Logger.info(`NDCG@10: ${metrics.ndcgAt10.toFixed(4)}`);
    Logger.info(`Success@1: ${(metrics.successAt1 * 100).toFixed(2)}%`);
    Logger.info(`Success@10: ${(metrics.successAt10 * 100).toFixed(2)}%`);
    Logger.info(`Found Rate (<= ${MAX_RANK_TO_CHECK}): ${(metrics.successRate * 100).toFixed(2)}%`);

    const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
    const datasetName = path.basename(datasetPath, '.json');
    const modelNameForFilename = config.OllamaModel.replace(/:/g, '.');

    // ---- REMOVED FILE WRITING FOR OPTIMIZATION RUN ----
    // try {
    //     const resultsFilename = path.join(OUTPUT_DIR, `eval_results_${datasetName}_${modelNameForFilename}_${timestamp}.json`);
    //     fs.writeFileSync(resultsFilename, JSON.stringify(evaluationResults, null, 2));
    //     Logger.info(`Detailed evaluation results saved to ${resultsFilename}`);

    //     const summaryFilename = path.join(OUTPUT_DIR, `summary_${datasetName}_${modelNameForFilename}_${timestamp}.txt`);
    //     const summaryContent = generateSummaryReport(metrics, totalQueriesEvaluated, dataset.length, analysisResults, datasetPath);
    //     fs.writeFileSync(summaryFilename, summaryContent);
    //     Logger.info(`Summary report saved to ${summaryFilename}`);
    //     Logger.info("--- Summary Report --- ");
    //     summaryContent.split('\n').forEach(line => Logger.info(line.trim()));

    //     if (analysisResults.length > 0) {
    //         const analysisDetailsFilename = path.join(OUTPUT_DIR, `analysis_details_${datasetName}_${modelNameForFilename}_${timestamp}.json`);
    //         fs.writeFileSync(analysisDetailsFilename, JSON.stringify(analysisResults, null, 2));
    //         Logger.info(`Detailed analysis results saved to ${analysisDetailsFilename}`);
    //     }
    // } catch (error) {
    //     Logger.error({ error }, "Failed to save one or more output files");
    // }
    // ---- END REMOVED FILE WRITING ----
    return metrics; // Return calculated metrics
}

// --- Added: Function to Optimize Alpha ---
async function optimizeAlpha(datasetPath: string): Promise<void> {
    Logger.info("--- Starting Alpha Optimization --- ");
    Logger.info(`Using dataset: ${datasetPath}`);

    const alphaValues = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]; // Define range to test
    const results: Array<{ alpha: number; metrics: Metrics | null }> = [];

    let bestAlpha = -1;
    let bestMetrics: Metrics | null = null;

    for (const alpha of alphaValues) {
        Logger.info(`--- Running evaluation for alpha = ${alpha} ---`);
        const currentMetrics = await evaluateFromDataset(datasetPath, alpha);
        results.push({ alpha, metrics: currentMetrics });

        if (currentMetrics) {
            Logger.info(` Alpha ${alpha}: MRR = ${currentMetrics.mrr.toFixed(4)}, NDCG@10 = ${currentMetrics.ndcgAt10.toFixed(4)}`);

            // Determine if current metrics are better than the best found so far
            // Prioritize NDCG@10, then MRR as tie-breaker
            let isBetter = false;
            if (!bestMetrics) {
                isBetter = true; // First successful run is the best so far
            } else {
                if (currentMetrics.ndcgAt10 > bestMetrics.ndcgAt10) {
                    isBetter = true;
                } else if (currentMetrics.ndcgAt10 === bestMetrics.ndcgAt10 && currentMetrics.mrr > bestMetrics.mrr) {
                    isBetter = true;
                }
            }

            if (isBetter) {
                Logger.info(` --> New best metrics found for alpha = ${alpha}`);
                bestMetrics = currentMetrics;
                bestAlpha = alpha;
            }
        } else {
             Logger.warn(` --> Evaluation failed for alpha = ${alpha}`);
        }
        Logger.info(`--- Finished evaluation for alpha = ${alpha} ---`);
    }

    Logger.info("--- Alpha Optimization Finished --- ");
    Logger.info("Optimization Summary:");
    results.forEach(res => {
        if (res.metrics) {
            Logger.info(`  Alpha ${res.alpha.toFixed(1)}: MRR = ${res.metrics.mrr.toFixed(4)}, NDCG@10 = ${res.metrics.ndcgAt10.toFixed(4)}`);
        } else {
             Logger.info(`  Alpha ${res.alpha.toFixed(1)}: Evaluation Failed`);
        }
    });

    if (bestAlpha !== -1 && bestMetrics) {
        Logger.info(`\nBEST Alpha Found: ${bestAlpha.toFixed(1)}`);
        Logger.info(`  Best MRR:     ${bestMetrics.mrr.toFixed(4)}`);
        Logger.info(`  Best NDCG@10: ${bestMetrics.ndcgAt10.toFixed(4)}`);
    } else {
        Logger.warn("\nNo successful evaluation run completed during optimization.");
    }
}
// --- End Optimize Alpha Function ---

// Main function to orchestrate the evaluation
async function mainEvaluationRouter() {
    // Add a check for OLLAMA_MODEL in config for 'generate' mode
    if (EVAL_MODE === 'generate') {
        // The check is now inside generateEvaluationDataset
        await generateEvaluationDataset();
    } else if (EVAL_MODE === 'evaluate') {
        if (!EVAL_DATASET_PATH) {
            Logger.error("Error: EVAL_MODE is 'evaluate' but EVAL_DATASET_PATH environment variable is not set.");
            Logger.error("Please set EVAL_DATASET_PATH to the path of the .json dataset file.");
            process.exit(1);
        }
        // When running 'evaluate' mode directly, use a default alpha (e.g., 0.5)
        // Or you could require an ALPHA env var here as well.
        const defaultAlpha = parseFloat(process.env.DEFAULT_ALPHA || "0.5");
        Logger.warn(`Running single evaluation with alpha=${defaultAlpha}. Set DEFAULT_ALPHA env var to change.`);
        const metrics = await evaluateFromDataset(EVAL_DATASET_PATH, defaultAlpha);
        // We need to manually generate the report here if needed, as evaluateFromDataset no longer does.
        if (metrics) {
            const timestamp = new Date().toISOString().replace(/[:.]/g, "-");
            const datasetName = path.basename(EVAL_DATASET_PATH, '.json');
            const modelNameForFilename = config.OllamaModel.replace(/:/g, '.');
            const summaryFilename = path.join(OUTPUT_DIR, `summary_${datasetName}_${modelNameForFilename}_alpha${defaultAlpha}_${timestamp}.txt`);
            // Analysis results are not available here unless we collect them somehow
            const summaryContent = generateSummaryReport(metrics, metrics.rankDistribution.not_found > 0 ? Object.values(metrics.rankDistribution).reduce((a,b)=>a+b) : 0 , 0, [], EVAL_DATASET_PATH);
            try {
                fs.writeFileSync(summaryFilename, summaryContent);
                Logger.info(`Summary report for alpha=${defaultAlpha} saved to ${summaryFilename}`);
                Logger.info("--- Summary Report (Evaluate Mode) --- ");
                summaryContent.split('\n').forEach(line => Logger.info(line.trim()));
            } catch (error) {
                 Logger.error({ error }, "Failed to save summary report file for evaluate mode.");
            }
        } else {
            Logger.error("Evaluation run failed, no metrics generated.");
        }
    } else if (EVAL_MODE === 'optimize') { // Added Optimize Mode
         if (!EVAL_DATASET_PATH) {
            Logger.error("Error: EVAL_MODE is 'optimize' but EVAL_DATASET_PATH environment variable is not set.");
            Logger.error("Please set EVAL_DATASET_PATH to the path of the .json dataset file.");
            process.exit(1);
        }
        await optimizeAlpha(EVAL_DATASET_PATH);
    } else {
        Logger.error(`Error: Invalid or missing EVAL_MODE environment variable.`);
        Logger.error(`Please set EVAL_MODE to 'generate', 'evaluate', or 'optimize'.`); // Updated help text
        if (EVAL_MODE) {
            Logger.error(`Current value: ${EVAL_MODE}`);
        }
        process.exit(1);
    }
}

// --- Script Entry Point ---
mainEvaluationRouter().catch((error) => {
  Logger.error({ error: error?.message }, "Unhandled error during main evaluation runner");
  process.exit(1);
}); 